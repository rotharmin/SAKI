{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAKI SS19  Homework 1 - Armin Roth\n",
    "## Transaction Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import datetime\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.cistem import Cistem\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize, MinMaxScaler\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc,roc_auc_score, classification_report, confusion_matrix, precision_recall_curve\n",
    "from schwifty import IBAN,BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "df = pd.read_csv(\"./Exercise 1 - Transaction Classification - Data Set.csv\",sep=';',index_col=0, encoding='utf8',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:\n",
      "(209, 11)\n",
      "\n",
      "features and types:\n",
      "Auftragskonto                        float64\n",
      "Buchungstag                           object\n",
      "Valutadatum                           object\n",
      "Buchungstext                          object\n",
      "Verwendungszweck                      object\n",
      "Beguenstigter/Zahlungspflichtiger     object\n",
      "Kontonummer                           object\n",
      "BLZ                                   object\n",
      "Betrag                                object\n",
      "Waehrung                              object\n",
      "label                                 object\n",
      "dtype: object\n",
      "\n",
      "is null:\n",
      "Auftragskonto                        41\n",
      "Buchungstag                           0\n",
      "Valutadatum                           0\n",
      "Buchungstext                          0\n",
      "Verwendungszweck                      0\n",
      "Beguenstigter/Zahlungspflichtiger     0\n",
      "Kontonummer                           1\n",
      "BLZ                                   1\n",
      "Betrag                                0\n",
      "Waehrung                              0\n",
      "label                                 0\n",
      "dtype: int64\n",
      "\n",
      "class names:\n",
      "['income' 'living' 'private' 'standardOfLiving' 'leisure' 'finance']\n",
      "\n",
      "class frequencies:\n",
      "leisure             65\n",
      "standardOfLiving    47\n",
      "finance             33\n",
      "living              26\n",
      "private             21\n",
      "income              17\n",
      "Name: label, dtype: int64\n",
      "\n",
      "unique values per class:\n",
      "Auftragskonto: 2\n",
      "Buchungstag: 85\n",
      "Valutadatum: 85\n",
      "Buchungstext: 14\n",
      "Verwendungszweck: 125\n",
      "Beguenstigter/Zahlungspflichtiger: 59\n",
      "Kontonummer: 53\n",
      "BLZ: 40\n",
      "Betrag: 94\n",
      "Waehrung: 1\n",
      "label: 6\n"
     ]
    }
   ],
   "source": [
    "# # # # # # # # # # # # # # # # # # # #\n",
    "# gather some information about dataset\n",
    "# # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "print('shape:')\n",
    "print(df.shape)\n",
    "print('\\nfeatures and types:')\n",
    "print(df.dtypes)\n",
    "print('\\nis null:')\n",
    "print(df.isnull().sum())\n",
    "print('\\nclass names:')\n",
    "print(pd.unique(df.label))\n",
    "print('\\nclass frequencies:')\n",
    "print(df.label.value_counts())\n",
    "print('\\nunique values per class:')\n",
    "for column in df:\n",
    "    print(column +': '+ str(df[column].nunique()))\n",
    "\n",
    "#df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # #\n",
    "# prepare Data in general\n",
    "# # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "#fill null Values with \"0\"\n",
    "df=df.fillna(0)\n",
    "\n",
    "#Convert Datatypes\n",
    "df['Kontonummer'] = df['Kontonummer'].astype('str')\n",
    "df['BLZ'] = df['BLZ'].astype('str')\n",
    "df['Buchungstag'] = pd.to_datetime(df['Buchungstag'])\n",
    "df['Betrag'] = [x.replace(',', '.') for x in df['Betrag']]\n",
    "df['Betrag'] = df['Betrag'].astype('float')\n",
    "#df['Auftragskonto'] = df['Auftragskonto'].astype('int')\n",
    "#df['Valutadatum'] = pd.to_datetime(df['Valutadatum'])\n",
    "\n",
    "#drop unused columns (low variance or duplicates)\n",
    "df = df.drop(['Auftragskonto','Waehrung','Valutadatum'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # #\n",
    "# standardize bankaccount\n",
    "# # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "#convert blz and account number to iban\n",
    "def prepare_bankaccount(kontonummer,blz):\n",
    "    if(len(kontonummer) == 0):\n",
    "        return ''\n",
    "    if(len(kontonummer) != 22):\n",
    "        if(len(blz) > 8):\n",
    "            blz = BIC(blz).country_bank_code\n",
    "        kontonummer = IBAN.generate('DE', bank_code=blz, account_code=kontonummer)\n",
    "    return str(kontonummer)\n",
    "\n",
    "#kontonummer vereinlichen\n",
    "df['iban'] = df.apply(lambda x: prepare_bankaccount(kontonummer=x['Kontonummer'],blz=x['BLZ']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # #\n",
    "# prepare text features\n",
    "# # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "#cleanup and tokenize text\n",
    "def prepare_text(text):\n",
    "    #convert all text to lower case\n",
    "    text = text.lower()\n",
    "    #remove punctuation \n",
    "    table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    text = text.translate(table)\n",
    "    #tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    #remove stop words\n",
    "    for token in tokens:\n",
    "        if token in stopwords.words('german'):\n",
    "            tokens.remove(token)\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "       \n",
    "#count vectorize and tfidf transform\n",
    "def vectorize_transform(prepared_text):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(prepared_text).toarray()\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_tfidf = tfidf_transformer.fit_transform(X).toarray()\n",
    "    topics = vectorizer.get_feature_names()\n",
    "    return pd.DataFrame(data=X_tfidf, columns=topics)\n",
    "\n",
    "#count vectorize\n",
    "def vectorize(prepared_text):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(prepared_text).toarray()\n",
    "    topics = vectorizer.get_feature_names()\n",
    "    return pd.DataFrame(data=X, columns=topics)\n",
    "\n",
    "#select k best features\n",
    "def k_best(x,y,k):\n",
    "    selector = SelectKBest(chi2, k=k)\n",
    "    x_new = selector.fit_transform(x,y)\n",
    "    columns = selector.get_support(indices=True)\n",
    "    new_columns = x.columns[columns]\n",
    "    return pd.DataFrame(x_new,columns=new_columns)\n",
    "\n",
    "\n",
    "#tokenize feaures\n",
    "Verwendungszweck_tokenized = df['Verwendungszweck'].apply(prepare_text)\n",
    "Buchungstext_tokenized = df['Buchungstext'].apply(prepare_text)\n",
    "\n",
    "#vecorize/transform feaures\n",
    "df_verwendungszweck = vectorize_transform(Verwendungszweck_tokenized)\n",
    "df_buchungstext = vectorize_transform(Buchungstext_tokenized)\n",
    "df_iban = vectorize(df['iban'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # #\n",
    "# prepare betrag\n",
    "# # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "#Normalize values\n",
    "x = df[['Betrag']].values\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df_betrag = pd.DataFrame(x_scaled, columns=['Betrag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # #\n",
    "# prepare buchungstag\n",
    "# # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "#df_Buchungstag_weekday=df['Buchungstag'].apply(lambda x: x.weekday())\n",
    "#df_Buchungstag_day=df['Buchungstag'].apply(lambda x: x.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # #\n",
    "# assambling final test/train dataset \n",
    "# # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "y = df['label']\n",
    "df_final = pd.concat([k_best(df_verwendungszweck,y,160),df_iban,df_buchungstext,df_betrag],axis=1)\n",
    "\n",
    "#drop duplicated columns\n",
    "df_final = df_final.loc[:,~df_final.columns.duplicated()]\n",
    "#df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9047619047619048\n",
      "\n",
      "Classification Report: \n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "         finance       0.93      0.93      0.93        14\n",
      "          income       0.86      1.00      0.92         6\n",
      "         leisure       0.88      1.00      0.93        21\n",
      "          living       0.75      0.60      0.67         5\n",
      "         private       1.00      0.83      0.91         6\n",
      "standardOfLiving       1.00      0.82      0.90        11\n",
      "\n",
      "       micro avg       0.90      0.90      0.90        63\n",
      "       macro avg       0.90      0.86      0.88        63\n",
      "    weighted avg       0.91      0.90      0.90        63\n",
      "\n",
      "Confusion Matrix:\n",
      "                       pre:pri  pre:fin  pre:lei  pre:inc  pre:sta  pre:liv\n",
      "true:private                 5        1        0        0        0        0\n",
      "true:finance                 0       13        0        1        0        0\n",
      "true:leisure                 0        0       21        0        0        0\n",
      "true:income                  0        0        0        6        0        0\n",
      "true:standardOfLiving        0        0        1        0        9        1\n",
      "true:living                  0        0        2        0        0        3\n"
     ]
    }
   ],
   "source": [
    "# # # # # # # # # # # # # # # # # # # #\n",
    "# training the classifier and evaluation\n",
    "# # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_final, y, test_size=0.3,random_state=12345)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df_final, y, test_size=0.3,random_state=int(time.time()))\n",
    "\n",
    "#gausian naive bayes\n",
    "gnb = GaussianNB()\n",
    "\n",
    "#Train classifier\n",
    "gnb.fit(X_train,y_train)\n",
    "\n",
    "#classify\n",
    "y_pred = gnb.predict(X_test)\n",
    "y_pred_proba = gnb.predict_proba(X_test)\n",
    "\n",
    "#evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: ' + str(accuracy) + '\\n')\n",
    "\n",
    "print('Classification Report: \\n')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "unique_label = pd.unique(y_test)\n",
    "print(pd.DataFrame(confusion_matrix(y_test, y_pred, labels=unique_label), \n",
    "                   index=['true:{:}'.format(x) for x in unique_label], \n",
    "                   columns=['pre:{:}'.format(x[:3]) for x in unique_label]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
