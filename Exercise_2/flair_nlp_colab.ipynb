{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sF_9MAOKPyAY"
   },
   "source": [
    "# Resume NER\n",
    "## Extract Information from Resumes using Named Entity Recognition\n",
    "---\n",
    "### Training the model\n",
    "In this part a model on our data is trained with Flair NLP  and the results are evaluated.\n",
    "\n",
    "Run this code on google colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2zuCubbF-AQc"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MXiOU9ihIHvX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/content/gdrive/My Drive/SAKI/data\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l8542ZPSnM_d"
   },
   "outputs": [],
   "source": [
    "! pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10152,
     "status": "ok",
     "timestamp": 1560878604033,
     "user": {
      "displayName": "Armin Roth",
      "photoUrl": "",
      "userId": "08591799635376506733"
     },
     "user_tz": -120
    },
    "id": "Ghp5-JZTRYOb",
    "outputId": "deec0b8f-4ab6-4972-fc3d-fe6c5ecf7d1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-18 17:23:16,078 Reading data from /content/gdrive/My Drive/SAKI/flair\n",
      "2019-06-18 17:23:16,079 Train: /content/gdrive/My Drive/SAKI/flair/train_res_bilou_nd.txt\n",
      "2019-06-18 17:23:16,085 Dev: /content/gdrive/My Drive/SAKI/flair/valid_res_bilou_nd.txt\n",
      "2019-06-18 17:23:16,086 Test: /content/gdrive/My Drive/SAKI/flair/test_res_bilou_nd.txt\n",
      "Corpus: 10199 train + 3120 dev + 2748 test sentences\n",
      "[b'<unk>', b'O', b'B-Designation', b'I-Designation', b'L-Designation', b'\"B-Companies', b'\"L-Companies', b'U-Degree', b'U-Designation', b'-', b'B-Degree', b'I-Degree', b'L-Degree', b'\"I-Companies', b'\"U-Companies', b'<START>', b'<STOP>']\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "## describes file structure\n",
    "columns = {0: 'text', 1: 'ner'}\n",
    "\n",
    "## folder where training and test data are\n",
    "data_folder = '/content/gdrive/My Drive/SAKI/flair'\n",
    "\n",
    "\n",
    "#train_file = 'train_res_bilou_nd.txt'\n",
    "#test_file = 'test_res_bilou_nd.txt'\n",
    "#dev_file = 'valid_res_bilou_nd.txt'\n",
    "train_file = 'train_res_bilou_f.txt'\n",
    "test_file = 'test_res_bilou_f.txt'\n",
    "dev_file = 'valid_res_bilou_f.txt'\n",
    "\n",
    "\n",
    "## init a corpus using column format, data folder and the names of the train, dev and test files\n",
    "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "                              train_file=train_file,\n",
    "                              test_file=test_file,\n",
    "                              dev_file=dev_file)\n",
    "print(corpus)\n",
    "\n",
    "## make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type='ner')\n",
    "print(tag_dictionary.idx2item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o6H1IzUbR5iH"
   },
   "outputs": [],
   "source": [
    "## initialize embeddings\n",
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings,FlairEmbeddings\n",
    "from typing import List\n",
    "\n",
    "embedding_types: List[TokenEmbeddings] = [\n",
    "    WordEmbeddings('glove'),\n",
    "    FlairEmbeddings('news-forward'),\n",
    "    FlairEmbeddings('news-backward'),\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "## initialize sequence tagger\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type='ner',\n",
    "                                        use_crf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3695,
     "status": "ok",
     "timestamp": 1560806810698,
     "user": {
      "displayName": "Armin Roth",
      "photoUrl": "",
      "userId": "08591799635376506733"
     },
     "user_tz": -120
    },
    "id": "c1dq-V_AvA2x",
    "outputId": "7c907965-4087-403c-9e93-8af3562719eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "## prepare hyperparameter optimization\n",
    "\n",
    "from hyperopt import hp\n",
    "from flair.hyperparameter.param_selection import SearchSpace, Parameter\n",
    "\n",
    "search_space = SearchSpace()\n",
    "search_space.add(\n",
    "        Parameter.EMBEDDINGS,\n",
    "        hp.choice,\n",
    "        options=[\n",
    "            StackedEmbeddings([WordEmbeddings(\"glove\")]),\n",
    "            StackedEmbeddings(\n",
    "                [\n",
    "                    WordEmbeddings(\"glove\"),\n",
    "                    FlairEmbeddings(\"news-forward\"),\n",
    "                    FlairEmbeddings(\"news-backward\"),\n",
    "                ]\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "search_space.add(Parameter.HIDDEN_SIZE, hp.choice, options=[32, 64, 128])\n",
    "search_space.add(Parameter.RNN_LAYERS, hp.choice, options=[1, 2])\n",
    "search_space.add(Parameter.DROPOUT, hp.uniform, low=0.0, high=0.5)\n",
    "search_space.add(Parameter.LEARNING_RATE, hp.choice, options=[0.05, 0.1, 0.15, 0.2])\n",
    "search_space.add(Parameter.MINI_BATCH_SIZE, hp.choice, options=[8, 16, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1666
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 752949,
     "status": "error",
     "timestamp": 1560807594696,
     "user": {
      "displayName": "Armin Roth",
      "photoUrl": "",
      "userId": "08591799635376506733"
     },
     "user_tz": -120
    },
    "id": "8irj5_nSQl9c",
    "outputId": "8475c004-3ed0-4c69-ae21-74690755941b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s, best loss: ?]2019-06-17 21:27:23,100 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-17 21:27:23,101 Evaluation run: 1\n",
      "2019-06-17 21:27:23,102 Evaluating parameter combination:\n",
      "2019-06-17 21:27:23,104 \tdropout: 0.15619438301333338\n",
      "2019-06-17 21:27:23,105 \tembeddings: StackedEmbeddings [/root/.flair/embeddings/glove.gensim]\n",
      "2019-06-17 21:27:23,105 \thidden_size: 128\n",
      "2019-06-17 21:27:23,106 \tlearning_rate: 0.1\n",
      "2019-06-17 21:27:23,107 \tmini_batch_size: 8\n",
      "2019-06-17 21:27:23,108 \trnn_layers: 2\n",
      "2019-06-17 21:27:23,109 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-17 21:27:23,291 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-17 21:27:23,292 Training run: 1\n",
      "2019-06-17 21:27:23,306 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-17 21:27:23,308 Evaluation method: MICRO_F1_SCORE\n",
      "2019-06-17 21:27:23,650 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-17 21:27:24,190 epoch 1 - iter 0/1275 - loss 64.91671753\n",
      "2019-06-17 21:27:44,375 epoch 1 - iter 127/1275 - loss 6.71708530\n",
      "2019-06-17 21:28:08,041 epoch 1 - iter 254/1275 - loss 5.61385957\n",
      "2019-06-17 21:28:25,120 epoch 1 - iter 381/1275 - loss 4.64062849\n",
      "2019-06-17 21:28:45,273 epoch 1 - iter 508/1275 - loss 4.13949142\n",
      "2019-06-17 21:29:04,834 epoch 1 - iter 635/1275 - loss 3.84127623\n",
      "2019-06-17 21:29:25,809 epoch 1 - iter 762/1275 - loss 3.60314604\n",
      "2019-06-17 21:29:46,286 epoch 1 - iter 889/1275 - loss 3.42429809\n",
      "2019-06-17 21:30:07,165 epoch 1 - iter 1016/1275 - loss 3.26068302\n",
      "2019-06-17 21:30:28,819 epoch 1 - iter 1143/1275 - loss 3.14845489\n",
      "2019-06-17 21:30:47,913 epoch 1 - iter 1270/1275 - loss 3.04879171\n",
      "2019-06-17 21:30:48,617 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-17 21:30:48,622 EPOCH 1 done: loss 3.0425 - lr 0.1000 - bad epochs 0\n",
      "2019-06-17 21:31:22,537 DEV : loss 1.8130191564559937 - score 0.4768\n",
      "2019-06-17 21:31:22,543 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-17 21:31:23,116 epoch 2 - iter 0/1275 - loss 0.97896194\n",
      "2019-06-17 21:31:44,987 epoch 2 - iter 127/1275 - loss 1.95485640\n",
      "2019-06-17 21:32:03,375 epoch 2 - iter 254/1275 - loss 1.95509416\n",
      "2019-06-17 21:32:25,500 epoch 2 - iter 381/1275 - loss 1.90389155\n",
      "2019-06-17 21:32:45,061 epoch 2 - iter 508/1275 - loss 1.92395332\n",
      "2019-06-17 21:33:08,362 epoch 2 - iter 635/1275 - loss 1.96345196\n",
      "2019-06-17 21:33:26,551 epoch 2 - iter 762/1275 - loss 1.94336769\n",
      "2019-06-17 21:33:50,577 epoch 2 - iter 889/1275 - loss 1.90001339\n",
      "2019-06-17 21:34:08,708 epoch 2 - iter 1016/1275 - loss 1.85106217\n",
      "2019-06-17 21:34:27,603 epoch 2 - iter 1143/1275 - loss 1.84463097\n",
      "2019-06-17 21:34:46,404 epoch 2 - iter 1270/1275 - loss 1.80901393\n",
      "2019-06-17 21:34:48,126 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-17 21:34:48,128 EPOCH 2 done: loss 1.8129 - lr 0.1000 - bad epochs 0\n",
      "2019-06-17 21:35:24,959 DEV : loss 1.5281765460968018 - score 0.5031\n",
      "2019-06-17 21:35:24,965 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-17 21:35:25,894 epoch 3 - iter 0/1275 - loss 0.97789812\n",
      "2019-06-17 21:35:43,594 epoch 3 - iter 127/1275 - loss 1.53196541\n",
      "2019-06-17 21:36:02,757 epoch 3 - iter 254/1275 - loss 1.54703151\n",
      "2019-06-17 21:36:25,704 epoch 3 - iter 381/1275 - loss 1.60246553\n",
      "2019-06-17 21:36:48,896 epoch 3 - iter 508/1275 - loss 1.59447138\n",
      "2019-06-17 21:37:09,181 epoch 3 - iter 635/1275 - loss 1.61132725\n",
      "2019-06-17 21:37:30,225 epoch 3 - iter 762/1275 - loss 1.58870712\n",
      "2019-06-17 21:37:47,273 epoch 3 - iter 889/1275 - loss 1.60492684\n",
      "2019-06-17 21:38:09,128 epoch 3 - iter 1016/1275 - loss 1.60567434\n",
      "2019-06-17 21:38:30,098 epoch 3 - iter 1143/1275 - loss 1.59431296\n",
      "2019-06-17 21:38:49,278 epoch 3 - iter 1270/1275 - loss 1.56951721\n",
      "2019-06-17 21:38:49,976 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-17 21:38:49,979 EPOCH 3 done: loss 1.5676 - lr 0.1000 - bad epochs 0\n",
      "2019-06-17 21:39:26,908 DEV : loss 1.3690073490142822 - score 0.5434\n",
      "2019-06-17 21:39:26,915 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-17 21:39:26,919 Testing using best model ...\n",
      "2019-06-17 21:39:54,877 0.656\t0.5135\t0.5761\n",
      "2019-06-17 21:39:54,879 \n",
      "MICRO_AVG: acc 0.4046 - f1-score 0.5761\n",
      "MACRO_AVG: acc 0.3016 - f1-score 0.4134\n",
      "\"B-Companies tp: 207 - fp: 110 - fn: 101 - tn: 207 - precision: 0.6530 - recall: 0.6721 - accuracy: 0.4952 - f1-score: 0.6624\n",
      "\"I-Companies tp: 382 - fp: 167 - fn: 111 - tn: 382 - precision: 0.6958 - recall: 0.7748 - accuracy: 0.5788 - f1-score: 0.7332\n",
      "\"L-Companies tp: 238 - fp: 79 - fn: 78 - tn: 238 - precision: 0.7508 - recall: 0.7532 - accuracy: 0.6025 - f1-score: 0.7520\n",
      "\"U-Companies tp: 5 - fp: 6 - fn: 85 - tn: 5 - precision: 0.4545 - recall: 0.0556 - accuracy: 0.0521 - f1-score: 0.0991\n",
      "-          tp: 0 - fp: 0 - fn: 442 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "Degree     tp: 31 - fp: 22 - fn: 42 - tn: 31 - precision: 0.5849 - recall: 0.4247 - accuracy: 0.3263 - f1-score: 0.4921\n",
      "Designation tp: 171 - fp: 159 - fn: 176 - tn: 171 - precision: 0.5182 - recall: 0.4928 - accuracy: 0.3379 - f1-score: 0.5052\n",
      "L-Degree   tp: 32 - fp: 20 - fn: 35 - tn: 32 - precision: 0.6154 - recall: 0.4776 - accuracy: 0.3678 - f1-score: 0.5378\n",
      "L-Designation tp: 222 - fp: 108 - fn: 106 - tn: 222 - precision: 0.6727 - recall: 0.6768 - accuracy: 0.5092 - f1-score: 0.6747\n",
      "U-Degree   tp: 1 - fp: 0 - fn: 20 - tn: 1 - precision: 1.0000 - recall: 0.0476 - accuracy: 0.0476 - f1-score: 0.0909\n",
      "U-Designation tp: 0 - fp: 5 - fn: 25 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "2019-06-17 21:39:54,882 ----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3ebfa87d0ced>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# start the optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/flair/hyperparameter/param_selection.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, space, max_evals)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0msearch_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         best = fmin(\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_objective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         )\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[1;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/flair/hyperparameter/param_selection.py\u001b[0m in \u001b[0;36m_objective\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_scores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mvars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mvar\u001b[0;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[1;32m   3365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3366\u001b[0m     return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n\u001b[0;32m-> 3367\u001b[0;31m                          **kwargs)\n\u001b[0m\u001b[1;32m   3368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_var\u001b[0;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 ret, rcount, out=ret, casting='unsafe', subok=False)\n\u001b[1;32m    131\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dtype'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'torch.dtype' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "## optimize hyperparameters\n",
    "\n",
    "from flair.hyperparameter.param_selection import SequenceTaggerParamSelector , OptimizationValue\n",
    "\n",
    "## create the parameter selector\n",
    "optimizer = SequenceTaggerParamSelector(\n",
    "        corpus, 'ner', 'resources/results', max_epochs=3\n",
    "    )\n",
    "\n",
    "## start the optimization\n",
    "optimizer.optimize(search_space, max_evals=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 14739
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 435394,
     "status": "ok",
     "timestamp": 1560773438712,
     "user": {
      "displayName": "Armin Roth",
      "photoUrl": "",
      "userId": "08591799635376506733"
     },
     "user_tz": -120
    },
    "id": "xFMA2qsyTvHq",
    "outputId": "b0f19988-fb5c-41aa-ab41-52edad082576"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-18 06:45:42,175 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 06:45:42,180 Evaluation method: MICRO_F1_SCORE\n",
      "2019-06-18 06:45:43,333 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 06:45:47,678 epoch 1 - iter 0/319 - loss 103.13024139\n",
      "2019-06-18 06:46:44,530 epoch 1 - iter 31/319 - loss 11.70843418\n",
      "2019-06-18 06:47:27,332 epoch 1 - iter 62/319 - loss 8.28453587\n",
      "2019-06-18 06:48:19,459 epoch 1 - iter 93/319 - loss 6.88372860\n",
      "2019-06-18 06:49:18,264 epoch 1 - iter 124/319 - loss 6.14227489\n",
      "2019-06-18 06:49:59,425 epoch 1 - iter 155/319 - loss 5.52124013\n",
      "2019-06-18 06:51:03,850 epoch 1 - iter 186/319 - loss 5.24744341\n",
      "2019-06-18 06:51:55,229 epoch 1 - iter 217/319 - loss 4.93243011\n",
      "2019-06-18 06:53:08,875 epoch 1 - iter 248/319 - loss 4.65407707\n",
      "2019-06-18 06:53:54,413 epoch 1 - iter 279/319 - loss 4.40063443\n",
      "2019-06-18 06:54:30,444 epoch 1 - iter 310/319 - loss 4.16598169\n",
      "2019-06-18 06:54:41,884 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 06:54:41,885 EPOCH 1 done: loss 4.1021 - lr 0.1000 - bad epochs 0\n",
      "2019-06-18 06:57:11,555 DEV : loss 2.1926283836364746 - score 0.3294\n",
      "2019-06-18 06:59:12,282 TEST : loss 2.231055974960327 - score 0.3514\n",
      "2019-06-18 06:59:29,337 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 06:59:31,519 epoch 2 - iter 0/319 - loss 4.13513899\n",
      "2019-06-18 07:00:17,909 epoch 2 - iter 31/319 - loss 2.51098252\n",
      "2019-06-18 07:01:09,917 epoch 2 - iter 62/319 - loss 2.21474265\n",
      "2019-06-18 07:01:56,420 epoch 2 - iter 93/319 - loss 2.08900622\n",
      "2019-06-18 07:02:59,645 epoch 2 - iter 124/319 - loss 2.12719677\n",
      "2019-06-18 07:04:02,868 epoch 2 - iter 155/319 - loss 2.10231308\n",
      "2019-06-18 07:04:51,987 epoch 2 - iter 186/319 - loss 2.07945877\n",
      "2019-06-18 07:05:48,345 epoch 2 - iter 217/319 - loss 2.07050380\n",
      "2019-06-18 07:06:28,893 epoch 2 - iter 248/319 - loss 2.01456027\n",
      "2019-06-18 07:07:14,411 epoch 2 - iter 279/319 - loss 1.95847071\n",
      "2019-06-18 07:08:10,097 epoch 2 - iter 310/319 - loss 1.93710338\n",
      "2019-06-18 07:08:20,232 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 07:08:20,235 EPOCH 2 done: loss 1.9311 - lr 0.1000 - bad epochs 0\n",
      "2019-06-18 07:10:47,763 DEV : loss 1.5850764513015747 - score 0.5517\n",
      "2019-06-18 07:12:48,034 TEST : loss 1.5742064714431763 - score 0.5842\n",
      "2019-06-18 07:12:59,090 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 07:13:00,896 epoch 3 - iter 0/319 - loss 0.53642988\n",
      "2019-06-18 07:13:46,663 epoch 3 - iter 31/319 - loss 1.80547851\n",
      "2019-06-18 07:14:38,016 epoch 3 - iter 62/319 - loss 1.75939326\n",
      "2019-06-18 07:15:33,919 epoch 3 - iter 93/319 - loss 1.64495501\n",
      "2019-06-18 07:16:31,410 epoch 3 - iter 124/319 - loss 1.64971052\n",
      "2019-06-18 07:17:26,006 epoch 3 - iter 155/319 - loss 1.63481435\n",
      "2019-06-18 07:18:16,628 epoch 3 - iter 186/319 - loss 1.63832255\n",
      "2019-06-18 07:19:14,367 epoch 3 - iter 217/319 - loss 1.62567372\n",
      "2019-06-18 07:20:20,643 epoch 3 - iter 248/319 - loss 1.63949155\n",
      "2019-06-18 07:20:59,026 epoch 3 - iter 279/319 - loss 1.61695209\n",
      "2019-06-18 07:21:47,454 epoch 3 - iter 310/319 - loss 1.57365878\n",
      "2019-06-18 07:21:56,512 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 07:21:56,516 EPOCH 3 done: loss 1.5649 - lr 0.1000 - bad epochs 0\n",
      "2019-06-18 07:24:24,064 DEV : loss 1.4324411153793335 - score 0.5727\n",
      "2019-06-18 07:26:23,717 TEST : loss 1.4326698780059814 - score 0.6082\n",
      "2019-06-18 07:26:35,605 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 07:26:39,631 epoch 4 - iter 0/319 - loss 1.94972742\n",
      "2019-06-18 07:27:23,265 epoch 4 - iter 31/319 - loss 1.39145613\n",
      "2019-06-18 07:28:10,159 epoch 4 - iter 62/319 - loss 1.32417761\n",
      "2019-06-18 07:29:09,802 epoch 4 - iter 93/319 - loss 1.46910659\n",
      "2019-06-18 07:29:57,721 epoch 4 - iter 124/319 - loss 1.42239926\n",
      "2019-06-18 07:30:41,011 epoch 4 - iter 155/319 - loss 1.36853600\n",
      "2019-06-18 07:31:32,073 epoch 4 - iter 186/319 - loss 1.36353545\n",
      "2019-06-18 07:32:24,229 epoch 4 - iter 217/319 - loss 1.39327805\n",
      "2019-06-18 07:33:24,298 epoch 4 - iter 248/319 - loss 1.39914156\n",
      "2019-06-18 07:34:16,496 epoch 4 - iter 279/319 - loss 1.39049126\n",
      "2019-06-18 07:35:14,242 epoch 4 - iter 310/319 - loss 1.36789545\n",
      "2019-06-18 07:35:23,810 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 07:35:23,813 EPOCH 4 done: loss 1.3582 - lr 0.1000 - bad epochs 0\n",
      "2019-06-18 07:37:52,431 DEV : loss 1.3311142921447754 - score 0.5727\n",
      "2019-06-18 07:39:51,438 TEST : loss 1.3522950410842896 - score 0.6008\n",
      "2019-06-18 07:40:03,069 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 07:40:05,635 epoch 5 - iter 0/319 - loss 1.67067850\n",
      "2019-06-18 07:40:42,892 epoch 5 - iter 31/319 - loss 1.23475197\n",
      "2019-06-18 07:41:27,878 epoch 5 - iter 62/319 - loss 1.30748431\n",
      "2019-06-18 07:42:13,611 epoch 5 - iter 93/319 - loss 1.24739694\n",
      "2019-06-18 07:42:52,190 epoch 5 - iter 124/319 - loss 1.21296194\n",
      "2019-06-18 07:43:46,271 epoch 5 - iter 155/319 - loss 1.25594965\n",
      "2019-06-18 07:44:53,729 epoch 5 - iter 186/319 - loss 1.25824077\n",
      "2019-06-18 07:45:39,950 epoch 5 - iter 217/319 - loss 1.25194057\n",
      "2019-06-18 07:46:53,545 epoch 5 - iter 248/319 - loss 1.27715019\n",
      "2019-06-18 07:47:37,321 epoch 5 - iter 279/319 - loss 1.26088950\n",
      "2019-06-18 07:48:45,923 epoch 5 - iter 310/319 - loss 1.23572286\n",
      "2019-06-18 07:48:57,124 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 07:48:57,125 EPOCH 5 done: loss 1.2348 - lr 0.1000 - bad epochs 1\n",
      "2019-06-18 07:51:24,694 DEV : loss 1.2388519048690796 - score 0.5951\n",
      "2019-06-18 07:53:23,638 TEST : loss 1.240910291671753 - score 0.6062\n",
      "2019-06-18 07:53:34,766 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 07:53:36,801 epoch 6 - iter 0/319 - loss 0.94241405\n",
      "2019-06-18 07:54:42,481 epoch 6 - iter 31/319 - loss 1.30431076\n",
      "2019-06-18 07:55:34,276 epoch 6 - iter 62/319 - loss 1.23341451\n",
      "2019-06-18 07:56:37,710 epoch 6 - iter 93/319 - loss 1.21652095\n",
      "2019-06-18 07:57:28,564 epoch 6 - iter 124/319 - loss 1.18771317\n",
      "2019-06-18 07:58:01,266 epoch 6 - iter 155/319 - loss 1.13404885\n",
      "2019-06-18 07:58:42,344 epoch 6 - iter 186/319 - loss 1.13466201\n",
      "2019-06-18 07:59:31,273 epoch 6 - iter 217/319 - loss 1.12312362\n",
      "2019-06-18 08:00:22,672 epoch 6 - iter 248/319 - loss 1.14665214\n",
      "2019-06-18 08:01:25,659 epoch 6 - iter 279/319 - loss 1.16534945\n",
      "2019-06-18 08:02:07,641 epoch 6 - iter 310/319 - loss 1.15183982\n",
      "2019-06-18 08:02:21,656 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 08:02:21,660 EPOCH 6 done: loss 1.1425 - lr 0.1000 - bad epochs 0\n",
      "2019-06-18 08:04:50,820 DEV : loss 1.1701935529708862 - score 0.5887\n",
      "2019-06-18 08:06:51,841 TEST : loss 1.2101186513900757 - score 0.6189\n",
      "2019-06-18 08:06:58,123 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 08:07:01,544 epoch 7 - iter 0/319 - loss 1.58856916\n",
      "2019-06-18 08:07:45,790 epoch 7 - iter 31/319 - loss 0.99342879\n",
      "2019-06-18 08:08:34,638 epoch 7 - iter 62/319 - loss 0.97855439\n",
      "2019-06-18 08:09:13,299 epoch 7 - iter 93/319 - loss 0.99254903\n",
      "2019-06-18 08:09:56,376 epoch 7 - iter 124/319 - loss 0.99793703\n",
      "2019-06-18 08:10:50,152 epoch 7 - iter 155/319 - loss 1.02481069\n",
      "2019-06-18 08:12:00,347 epoch 7 - iter 186/319 - loss 1.05514711\n",
      "2019-06-18 08:13:09,083 epoch 7 - iter 217/319 - loss 1.06047516\n",
      "2019-06-18 08:13:57,945 epoch 7 - iter 248/319 - loss 1.05666302\n",
      "2019-06-18 08:15:04,058 epoch 7 - iter 279/319 - loss 1.06558721\n",
      "2019-06-18 08:15:45,504 epoch 7 - iter 310/319 - loss 1.05532686\n",
      "2019-06-18 08:15:54,807 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 08:15:54,810 EPOCH 7 done: loss 1.0492 - lr 0.1000 - bad epochs 1\n",
      "2019-06-18 08:18:23,472 DEV : loss 1.141228437423706 - score 0.6125\n",
      "2019-06-18 08:20:23,309 TEST : loss 1.214877724647522 - score 0.6187\n",
      "2019-06-18 08:20:34,376 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 08:20:36,499 epoch 8 - iter 0/319 - loss 0.24476027\n",
      "2019-06-18 08:21:27,194 epoch 8 - iter 31/319 - loss 0.97715988\n",
      "2019-06-18 08:22:20,788 epoch 8 - iter 62/319 - loss 1.01321725\n",
      "2019-06-18 08:23:22,699 epoch 8 - iter 93/319 - loss 0.99946062\n",
      "2019-06-18 08:24:07,237 epoch 8 - iter 124/319 - loss 1.04006649\n",
      "2019-06-18 08:25:00,575 epoch 8 - iter 155/319 - loss 1.01846297\n",
      "2019-06-18 08:25:54,775 epoch 8 - iter 186/319 - loss 1.02067902\n",
      "2019-06-18 08:26:29,657 epoch 8 - iter 217/319 - loss 1.01353476\n",
      "2019-06-18 08:27:40,445 epoch 8 - iter 248/319 - loss 1.00606728\n",
      "2019-06-18 08:28:27,283 epoch 8 - iter 279/319 - loss 1.01305572\n",
      "2019-06-18 08:29:10,586 epoch 8 - iter 310/319 - loss 1.01583968\n",
      "2019-06-18 08:29:23,666 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 08:29:23,667 EPOCH 8 done: loss 1.0176 - lr 0.1000 - bad epochs 0\n",
      "2019-06-18 08:31:50,589 DEV : loss 1.302648663520813 - score 0.5045\n",
      "2019-06-18 08:33:48,819 TEST : loss 1.3192929029464722 - score 0.5694\n",
      "2019-06-18 08:33:54,371 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 08:33:57,019 epoch 9 - iter 0/319 - loss 2.00350666\n",
      "2019-06-18 08:34:50,886 epoch 9 - iter 31/319 - loss 0.96022929\n",
      "2019-06-18 08:35:31,818 epoch 9 - iter 62/319 - loss 0.89718678\n",
      "2019-06-18 08:36:33,955 epoch 9 - iter 93/319 - loss 0.93536356\n",
      "2019-06-18 08:37:16,348 epoch 9 - iter 124/319 - loss 0.96192364\n",
      "2019-06-18 08:38:08,586 epoch 9 - iter 155/319 - loss 0.96394044\n",
      "2019-06-18 08:38:59,645 epoch 9 - iter 186/319 - loss 0.98130621\n",
      "2019-06-18 08:40:02,876 epoch 9 - iter 217/319 - loss 0.98436429\n",
      "2019-06-18 08:40:52,690 epoch 9 - iter 248/319 - loss 0.98227348\n",
      "2019-06-18 08:41:43,676 epoch 9 - iter 279/319 - loss 0.97421139\n",
      "2019-06-18 08:42:25,783 epoch 9 - iter 310/319 - loss 0.96908854\n",
      "2019-06-18 08:42:36,999 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 08:42:37,001 EPOCH 9 done: loss 0.9648 - lr 0.1000 - bad epochs 1\n",
      "2019-06-18 08:45:02,513 DEV : loss 1.1382641792297363 - score 0.6222\n",
      "2019-06-18 08:46:59,635 TEST : loss 1.1937438249588013 - score 0.6231\n",
      "2019-06-18 08:47:10,665 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 08:47:12,146 epoch 10 - iter 0/319 - loss 0.84789062\n",
      "2019-06-18 08:47:50,998 epoch 10 - iter 31/319 - loss 0.95123751\n",
      "2019-06-18 08:48:42,977 epoch 10 - iter 62/319 - loss 0.96892219\n",
      "2019-06-18 08:49:37,729 epoch 10 - iter 93/319 - loss 0.90663730\n",
      "2019-06-18 08:50:18,306 epoch 10 - iter 124/319 - loss 0.92248317\n",
      "2019-06-18 08:51:05,152 epoch 10 - iter 155/319 - loss 0.92818531\n",
      "2019-06-18 08:52:07,394 epoch 10 - iter 186/319 - loss 0.92468735\n",
      "2019-06-18 08:53:19,616 epoch 10 - iter 217/319 - loss 0.94115039\n",
      "2019-06-18 08:54:02,518 epoch 10 - iter 248/319 - loss 0.94719333\n",
      "2019-06-18 08:54:52,476 epoch 10 - iter 279/319 - loss 0.96155128\n",
      "2019-06-18 08:55:33,903 epoch 10 - iter 310/319 - loss 0.94318957\n",
      "2019-06-18 08:55:45,159 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 08:55:45,160 EPOCH 10 done: loss 0.9345 - lr 0.1000 - bad epochs 0\n",
      "2019-06-18 08:58:10,739 DEV : loss 1.1596516370773315 - score 0.6149\n",
      "2019-06-18 09:00:08,075 TEST : loss 1.2204002141952515 - score 0.6262\n",
      "2019-06-18 09:00:13,558 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 09:00:16,000 epoch 11 - iter 0/319 - loss 1.52644551\n",
      "2019-06-18 09:01:02,742 epoch 11 - iter 31/319 - loss 1.04981118\n",
      "2019-06-18 09:01:46,414 epoch 11 - iter 62/319 - loss 0.96322462\n",
      "2019-06-18 09:02:34,169 epoch 11 - iter 93/319 - loss 0.94964670\n",
      "2019-06-18 09:03:48,904 epoch 11 - iter 124/319 - loss 0.98216182\n",
      "2019-06-18 09:04:53,509 epoch 11 - iter 155/319 - loss 0.94416445\n",
      "2019-06-18 09:05:52,430 epoch 11 - iter 186/319 - loss 0.93610689\n",
      "2019-06-18 09:06:48,406 epoch 11 - iter 217/319 - loss 0.93453334\n",
      "2019-06-18 09:07:33,080 epoch 11 - iter 248/319 - loss 0.93058028\n",
      "2019-06-18 09:08:16,221 epoch 11 - iter 279/319 - loss 0.91901803\n",
      "2019-06-18 09:08:55,947 epoch 11 - iter 310/319 - loss 0.90979548\n",
      "2019-06-18 09:09:03,994 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 09:09:03,995 EPOCH 11 done: loss 0.9026 - lr 0.1000 - bad epochs 1\n",
      "2019-06-18 09:11:29,132 DEV : loss 1.1584479808807373 - score 0.6193\n",
      "2019-06-18 09:13:28,582 TEST : loss 1.203538417816162 - score 0.6168\n",
      "2019-06-18 09:13:33,913 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 09:13:35,613 epoch 12 - iter 0/319 - loss 0.93320858\n",
      "2019-06-18 09:14:28,018 epoch 12 - iter 31/319 - loss 0.81456022\n",
      "2019-06-18 09:15:20,068 epoch 12 - iter 62/319 - loss 0.95507089\n",
      "2019-06-18 09:16:14,005 epoch 12 - iter 93/319 - loss 0.90524415\n",
      "2019-06-18 09:17:08,912 epoch 12 - iter 124/319 - loss 0.88785790\n",
      "2019-06-18 09:17:46,587 epoch 12 - iter 155/319 - loss 0.85788610\n",
      "2019-06-18 09:18:42,145 epoch 12 - iter 186/319 - loss 0.84381055\n",
      "2019-06-18 09:19:32,042 epoch 12 - iter 217/319 - loss 0.85548664\n",
      "2019-06-18 09:20:17,659 epoch 12 - iter 248/319 - loss 0.86251563\n",
      "2019-06-18 09:21:12,419 epoch 12 - iter 279/319 - loss 0.86268496\n",
      "2019-06-18 09:22:04,965 epoch 12 - iter 310/319 - loss 0.86185759\n",
      "2019-06-18 09:22:16,003 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 09:22:16,007 EPOCH 12 done: loss 0.8608 - lr 0.1000 - bad epochs 2\n",
      "2019-06-18 09:24:41,324 DEV : loss 1.1402153968811035 - score 0.6091\n",
      "2019-06-18 09:26:40,667 TEST : loss 1.1709774732589722 - score 0.6172\n",
      "2019-06-18 09:26:46,045 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 09:26:47,474 epoch 13 - iter 0/319 - loss 0.17051271\n",
      "2019-06-18 09:27:51,430 epoch 13 - iter 31/319 - loss 0.79424806\n",
      "2019-06-18 09:28:37,736 epoch 13 - iter 62/319 - loss 0.79235868\n",
      "2019-06-18 09:29:24,744 epoch 13 - iter 93/319 - loss 0.78483182\n",
      "2019-06-18 09:30:13,474 epoch 13 - iter 124/319 - loss 0.78543675\n",
      "2019-06-18 09:30:59,524 epoch 13 - iter 155/319 - loss 0.79256120\n",
      "2019-06-18 09:32:01,711 epoch 13 - iter 186/319 - loss 0.81330235\n",
      "2019-06-18 09:32:54,133 epoch 13 - iter 217/319 - loss 0.82787033\n",
      "2019-06-18 09:33:34,305 epoch 13 - iter 248/319 - loss 0.82378553\n",
      "2019-06-18 09:34:30,321 epoch 13 - iter 279/319 - loss 0.82788363\n",
      "2019-06-18 09:35:16,790 epoch 13 - iter 310/319 - loss 0.83495782\n",
      "2019-06-18 09:35:26,438 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 09:35:26,440 EPOCH 13 done: loss 0.8257 - lr 0.1000 - bad epochs 3\n",
      "2019-06-18 09:37:52,062 DEV : loss 1.0886269807815552 - score 0.6129\n",
      "2019-06-18 09:39:50,779 TEST : loss 1.1506346464157104 - score 0.601\n",
      "Epoch    12: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2019-06-18 09:39:56,231 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 09:39:58,694 epoch 14 - iter 0/319 - loss 1.29366815\n",
      "2019-06-18 09:40:45,675 epoch 14 - iter 31/319 - loss 0.90800683\n",
      "2019-06-18 09:41:28,681 epoch 14 - iter 62/319 - loss 0.80589686\n",
      "2019-06-18 09:42:45,576 epoch 14 - iter 93/319 - loss 0.82068096\n",
      "2019-06-18 09:43:30,292 epoch 14 - iter 124/319 - loss 0.82379701\n",
      "2019-06-18 09:44:12,662 epoch 14 - iter 155/319 - loss 0.79185109\n",
      "2019-06-18 09:45:00,092 epoch 14 - iter 186/319 - loss 0.78395755\n",
      "2019-06-18 06:45:42,175 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 06:45:42,180 Evaluation method: MICRO_F1_SCORE\n",
      "2019-06-18 06:45:43,333 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 06:45:47,678 epoch 1 - iter 0/319 - loss 103.13024139\n",
      "2019-06-18 06:46:44,530 epoch 1 - iter 31/319 - loss 11.70843418\n",
      "2019-06-18 06:47:27,332 epoch 1 - iter 62/319 - loss 8.28453587\n",
      "2019-06-18 06:48:19,459 epoch 1 - iter 93/319 - loss 6.88372860\n",
      "2019-06-18 06:49:18,264 epoch 1 - iter 124/319 - loss 6.14227489\n",
      "2019-06-18 06:49:59,425 epoch 1 - iter 155/319 - loss 5.52124013\n",
      "2019-06-18 06:51:03,850 epoch 1 - iter 186/319 - loss 5.24744341\n",
      "2019-06-18 06:51:55,229 epoch 1 - iter 217/319 - loss 4.93243011\n",
      "2019-06-18 06:53:08,875 epoch 1 - iter 248/319 - loss 4.65407707\n",
      "2019-06-18 06:53:54,413 epoch 1 - iter 279/319 - loss 4.40063443\n",
      "2019-06-18 06:54:30,444 epoch 1 - iter 310/319 - loss 4.16598169\n",
      "2019-06-18 06:54:41,884 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 06:54:41,885 EPOCH 1 done: loss 4.1021 - lr 0.1000 - bad epochs 0\n",
      "2019-06-18 06:57:11,555 DEV : loss 2.1926283836364746 - score 0.3294\n",
      "2019-06-18 06:59:12,282 TEST : loss 2.231055974960327 - score 0.3514\n",
      "2019-06-18 06:59:29,337 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 06:59:31,519 epoch 2 - iter 0/319 - loss 4.13513899\n",
      "2019-06-18 07:00:17,909 epoch 2 - iter 31/319 - loss 2.51098252\n",
      "2019-06-18 07:01:09,917 epoch 2 - iter 62/319 - loss 2.21474265\n",
      "2019-06-18 07:01:56,420 epoch 2 - iter 93/319 - loss 2.08900622\n",
      "2019-06-18 07:02:59,645 epoch 2 - iter 124/319 - loss 2.12719677\n",
      "2019-06-18 07:04:02,868 epoch 2 - iter 155/319 - loss 2.10231308\n",
      "2019-06-18 07:04:51,987 epoch 2 - iter 186/319 - loss 2.07945877\n",
      "2019-06-18 07:05:48,345 epoch 2 - iter 217/319 - loss 2.07050380\n",
      "2019-06-18 07:06:28,893 epoch 2 - iter 248/319 - loss 2.01456027\n",
      "2019-06-18 07:07:14,411 epoch 2 - iter 279/319 - loss 1.95847071\n",
      "2019-06-18 07:08:10,097 epoch 2 - iter 310/319 - loss 1.93710338\n",
      "2019-06-18 07:08:20,232 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 07:08:20,235 EPOCH 2 done: loss 1.9311 - lr 0.1000 - bad epochs 0\n",
      "2019-06-18 07:10:47,763 DEV : loss 1.5850764513015747 - score 0.5517\n",
      "2019-06-18 07:12:48,034 TEST : loss 1.5742064714431763 - score 0.5842\n",
      "2019-06-18 07:12:59,090 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 07:13:00,896 epoch 3 - iter 0/319 - loss 0.53642988\n",
      "2019-06-18 07:13:46,663 epoch 3 - iter 31/319 - loss 1.80547851\n",
      "2019-06-18 07:14:38,016 epoch 3 - iter 62/319 - loss 1.75939326\n",
      "2019-06-18 07:15:33,919 epoch 3 - iter 93/319 - loss 1.64495501\n",
      "2019-06-18 07:16:31,410 epoch 3 - iter 124/319 - loss 1.64971052\n",
      "2019-06-18 07:17:26,006 epoch 3 - iter 155/319 - loss 1.63481435\n",
      "2019-06-18 07:18:16,628 epoch 3 - iter 186/319 - loss 1.63832255\n",
      "2019-06-18 07:19:14,367 epoch 3 - iter 217/319 - loss 1.62567372\n",
      "2019-06-18 07:20:20,643 epoch 3 - iter 248/319 - loss 1.63949155\n",
      "2019-06-18 07:20:59,026 epoch 3 - iter 279/319 - loss 1.61695209\n",
      "2019-06-18 07:21:47,454 epoch 3 - iter 310/319 - loss 1.57365878\n",
      "2019-06-18 07:21:56,512 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 07:21:56,516 EPOCH 3 done: loss 1.5649 - lr 0.1000 - bad epochs 0\n",
      "2019-06-18 07:24:24,064 DEV : loss 1.4324411153793335 - score 0.5727\n",
      "2019-06-18 07:26:23,717 TEST : loss 1.4326698780059814 - score 0.6082\n",
      "2019-06-18 07:26:35,605 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 07:26:39,631 epoch 4 - iter 0/319 - loss 1.94972742\n",
      "2019-06-18 07:27:23,265 epoch 4 - iter 31/319 - loss 1.39145613\n",
      "2019-06-18 07:28:10,159 epoch 4 - iter 62/319 - loss 1.32417761\n",
      "2019-06-18 07:29:09,802 epoch 4 - iter 93/319 - loss 1.46910659\n",
      "2019-06-18 07:29:57,721 epoch 4 - iter 124/319 - loss 1.42239926\n",
      "2019-06-18 07:30:41,011 epoch 4 - iter 155/319 - loss 1.36853600\n",
      "2019-06-18 07:31:32,073 epoch 4 - iter 186/319 - loss 1.36353545\n",
      "2019-06-18 07:32:24,229 epoch 4 - iter 217/319 - loss 1.39327805\n",
      "2019-06-18 07:33:24,298 epoch 4 - iter 248/319 - loss 1.39914156\n",
      "2019-06-18 07:34:16,496 epoch 4 - iter 279/319 - loss 1.39049126\n",
      "2019-06-18 07:35:14,242 epoch 4 - iter 310/319 - loss 1.36789545\n",
      "2019-06-18 07:35:23,810 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 07:35:23,813 EPOCH 4 done: loss 1.3582 - lr 0.1000 - bad epochs 0\n",
      "2019-06-18 07:37:52,431 DEV : loss 1.3311142921447754 - score 0.5727\n",
      "2019-06-18 07:39:51,438 TEST : loss 1.3522950410842896 - score 0.6008\n",
      "2019-06-18 07:40:03,069 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 07:40:05,635 epoch 5 - iter 0/319 - loss 1.67067850\n",
      "2019-06-18 07:40:42,892 epoch 5 - iter 31/319 - loss 1.23475197\n",
      "2019-06-18 07:41:27,878 epoch 5 - iter 62/319 - loss 1.30748431\n",
      "2019-06-18 07:42:13,611 epoch 5 - iter 93/319 - loss 1.24739694\n",
      "2019-06-18 07:42:52,190 epoch 5 - iter 124/319 - loss 1.21296194\n",
      "2019-06-18 07:43:46,271 epoch 5 - iter 155/319 - loss 1.25594965\n",
      "2019-06-18 07:44:53,729 epoch 5 - iter 186/319 - loss 1.25824077\n",
      "2019-06-18 07:45:39,950 epoch 5 - iter 217/319 - loss 1.25194057\n",
      "2019-06-18 07:46:53,545 epoch 5 - iter 248/319 - loss 1.27715019\n",
      "2019-06-18 07:47:37,321 epoch 5 - iter 279/319 - loss 1.26088950\n",
      "2019-06-18 07:48:45,923 epoch 5 - iter 310/319 - loss 1.23572286\n",
      "2019-06-18 07:48:57,124 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 07:48:57,125 EPOCH 5 done: loss 1.2348 - lr 0.1000 - bad epochs 1\n",
      "2019-06-18 07:51:24,694 DEV : loss 1.2388519048690796 - score 0.5951\n",
      "2019-06-18 07:53:23,638 TEST : loss 1.240910291671753 - score 0.6062\n",
      "2019-06-18 07:53:34,766 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 07:53:36,801 epoch 6 - iter 0/319 - loss 0.94241405\n",
      "2019-06-18 07:54:42,481 epoch 6 - iter 31/319 - loss 1.30431076\n",
      "2019-06-18 07:55:34,276 epoch 6 - iter 62/319 - loss 1.23341451\n",
      "2019-06-18 07:56:37,710 epoch 6 - iter 93/319 - loss 1.21652095\n",
      "2019-06-18 07:57:28,564 epoch 6 - iter 124/319 - loss 1.18771317\n",
      "2019-06-18 07:58:01,266 epoch 6 - iter 155/319 - loss 1.13404885\n",
      "2019-06-18 07:58:42,344 epoch 6 - iter 186/319 - loss 1.13466201\n",
      "2019-06-18 07:59:31,273 epoch 6 - iter 217/319 - loss 1.12312362\n",
      "2019-06-18 08:00:22,672 epoch 6 - iter 248/319 - loss 1.14665214\n",
      "2019-06-18 08:01:25,659 epoch 6 - iter 279/319 - loss 1.16534945\n",
      "2019-06-18 08:02:07,641 epoch 6 - iter 310/319 - loss 1.15183982\n",
      "2019-06-18 08:02:21,656 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 08:02:21,660 EPOCH 6 done: loss 1.1425 - lr 0.1000 - bad epochs 0\n",
      "2019-06-18 08:04:50,820 DEV : loss 1.1701935529708862 - score 0.5887\n",
      "2019-06-18 08:06:51,841 TEST : loss 1.2101186513900757 - score 0.6189\n",
      "2019-06-18 08:06:58,123 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 08:07:01,544 epoch 7 - iter 0/319 - loss 1.58856916\n",
      "2019-06-18 08:07:45,790 epoch 7 - iter 31/319 - loss 0.99342879\n",
      "2019-06-18 08:08:34,638 epoch 7 - iter 62/319 - loss 0.97855439\n",
      "2019-06-18 08:09:13,299 epoch 7 - iter 93/319 - loss 0.99254903\n",
      "2019-06-18 08:09:56,376 epoch 7 - iter 124/319 - loss 0.99793703\n",
      "2019-06-18 08:10:50,152 epoch 7 - iter 155/319 - loss 1.02481069\n",
      "2019-06-18 08:12:00,347 epoch 7 - iter 186/319 - loss 1.05514711\n",
      "2019-06-18 08:13:09,083 epoch 7 - iter 217/319 - loss 1.06047516\n",
      "2019-06-18 08:13:57,945 epoch 7 - iter 248/319 - loss 1.05666302\n",
      "2019-06-18 08:15:04,058 epoch 7 - iter 279/319 - loss 1.06558721\n",
      "2019-06-18 08:15:45,504 epoch 7 - iter 310/319 - loss 1.05532686\n",
      "2019-06-18 08:15:54,807 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 08:15:54,810 EPOCH 7 done: loss 1.0492 - lr 0.1000 - bad epochs 1\n",
      "2019-06-18 08:18:23,472 DEV : loss 1.141228437423706 - score 0.6125\n",
      "2019-06-18 08:20:23,309 TEST : loss 1.214877724647522 - score 0.6187\n",
      "2019-06-18 08:20:34,376 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 08:20:36,499 epoch 8 - iter 0/319 - loss 0.24476027\n",
      "2019-06-18 08:21:27,194 epoch 8 - iter 31/319 - loss 0.97715988\n",
      "2019-06-18 08:22:20,788 epoch 8 - iter 62/319 - loss 1.01321725\n",
      "2019-06-18 08:23:22,699 epoch 8 - iter 93/319 - loss 0.99946062\n",
      "2019-06-18 08:24:07,237 epoch 8 - iter 124/319 - loss 1.04006649\n",
      "2019-06-18 08:25:00,575 epoch 8 - iter 155/319 - loss 1.01846297\n",
      "2019-06-18 08:25:54,775 epoch 8 - iter 186/319 - loss 1.02067902\n",
      "2019-06-18 08:26:29,657 epoch 8 - iter 217/319 - loss 1.01353476\n",
      "2019-06-18 08:27:40,445 epoch 8 - iter 248/319 - loss 1.00606728\n",
      "2019-06-18 08:28:27,283 epoch 8 - iter 279/319 - loss 1.01305572\n",
      "2019-06-18 08:29:10,586 epoch 8 - iter 310/319 - loss 1.01583968\n",
      "2019-06-18 08:29:23,666 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 08:29:23,667 EPOCH 8 done: loss 1.0176 - lr 0.1000 - bad epochs 0\n",
      "2019-06-18 08:31:50,589 DEV : loss 1.302648663520813 - score 0.5045\n",
      "2019-06-18 08:33:48,819 TEST : loss 1.3192929029464722 - score 0.5694\n",
      "2019-06-18 08:33:54,371 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 08:33:57,019 epoch 9 - iter 0/319 - loss 2.00350666\n",
      "2019-06-18 08:34:50,886 epoch 9 - iter 31/319 - loss 0.96022929\n",
      "2019-06-18 08:35:31,818 epoch 9 - iter 62/319 - loss 0.89718678\n",
      "2019-06-18 08:36:33,955 epoch 9 - iter 93/319 - loss 0.93536356\n",
      "2019-06-18 08:37:16,348 epoch 9 - iter 124/319 - loss 0.96192364\n",
      "2019-06-18 08:38:08,586 epoch 9 - iter 155/319 - loss 0.96394044\n",
      "2019-06-18 08:38:59,645 epoch 9 - iter 186/319 - loss 0.98130621\n",
      "2019-06-18 08:40:02,876 epoch 9 - iter 217/319 - loss 0.98436429\n",
      "2019-06-18 08:40:52,690 epoch 9 - iter 248/319 - loss 0.98227348\n",
      "2019-06-18 08:41:43,676 epoch 9 - iter 279/319 - loss 0.97421139\n",
      "2019-06-18 08:42:25,783 epoch 9 - iter 310/319 - loss 0.96908854\n",
      "2019-06-18 08:42:36,999 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 08:42:37,001 EPOCH 9 done: loss 0.9648 - lr 0.1000 - bad epochs 1\n",
      "2019-06-18 08:45:02,513 DEV : loss 1.1382641792297363 - score 0.6222\n",
      "2019-06-18 08:46:59,635 TEST : loss 1.1937438249588013 - score 0.6231\n",
      "2019-06-18 08:47:10,665 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 08:47:12,146 epoch 10 - iter 0/319 - loss 0.84789062\n",
      "2019-06-18 08:47:50,998 epoch 10 - iter 31/319 - loss 0.95123751\n",
      "2019-06-18 08:48:42,977 epoch 10 - iter 62/319 - loss 0.96892219\n",
      "2019-06-18 08:49:37,729 epoch 10 - iter 93/319 - loss 0.90663730\n",
      "2019-06-18 08:50:18,306 epoch 10 - iter 124/319 - loss 0.92248317\n",
      "2019-06-18 08:51:05,152 epoch 10 - iter 155/319 - loss 0.92818531\n",
      "2019-06-18 08:52:07,394 epoch 10 - iter 186/319 - loss 0.92468735\n",
      "2019-06-18 08:53:19,616 epoch 10 - iter 217/319 - loss 0.94115039\n",
      "2019-06-18 08:54:02,518 epoch 10 - iter 248/319 - loss 0.94719333\n",
      "2019-06-18 08:54:52,476 epoch 10 - iter 279/319 - loss 0.96155128\n",
      "2019-06-18 08:55:33,903 epoch 10 - iter 310/319 - loss 0.94318957\n",
      "2019-06-18 08:55:45,159 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 08:55:45,160 EPOCH 10 done: loss 0.9345 - lr 0.1000 - bad epochs 0\n",
      "2019-06-18 08:58:10,739 DEV : loss 1.1596516370773315 - score 0.6149\n",
      "2019-06-18 09:00:08,075 TEST : loss 1.2204002141952515 - score 0.6262\n",
      "2019-06-18 09:00:13,558 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 09:00:16,000 epoch 11 - iter 0/319 - loss 1.52644551\n",
      "2019-06-18 09:01:02,742 epoch 11 - iter 31/319 - loss 1.04981118\n",
      "2019-06-18 09:01:46,414 epoch 11 - iter 62/319 - loss 0.96322462\n",
      "2019-06-18 09:02:34,169 epoch 11 - iter 93/319 - loss 0.94964670\n",
      "2019-06-18 09:03:48,904 epoch 11 - iter 124/319 - loss 0.98216182\n",
      "2019-06-18 09:04:53,509 epoch 11 - iter 155/319 - loss 0.94416445\n",
      "2019-06-18 09:05:52,430 epoch 11 - iter 186/319 - loss 0.93610689\n",
      "2019-06-18 09:06:48,406 epoch 11 - iter 217/319 - loss 0.93453334\n",
      "2019-06-18 09:07:33,080 epoch 11 - iter 248/319 - loss 0.93058028\n",
      "2019-06-18 09:08:16,221 epoch 11 - iter 279/319 - loss 0.91901803\n",
      "2019-06-18 09:08:55,947 epoch 11 - iter 310/319 - loss 0.90979548\n",
      "2019-06-18 09:09:03,994 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 09:09:03,995 EPOCH 11 done: loss 0.9026 - lr 0.1000 - bad epochs 1\n",
      "2019-06-18 09:11:29,132 DEV : loss 1.1584479808807373 - score 0.6193\n",
      "2019-06-18 09:13:28,582 TEST : loss 1.203538417816162 - score 0.6168\n",
      "2019-06-18 09:13:33,913 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 09:13:35,613 epoch 12 - iter 0/319 - loss 0.93320858\n",
      "2019-06-18 09:14:28,018 epoch 12 - iter 31/319 - loss 0.81456022\n",
      "2019-06-18 09:15:20,068 epoch 12 - iter 62/319 - loss 0.95507089\n",
      "2019-06-18 09:16:14,005 epoch 12 - iter 93/319 - loss 0.90524415\n",
      "2019-06-18 09:17:08,912 epoch 12 - iter 124/319 - loss 0.88785790\n",
      "2019-06-18 09:17:46,587 epoch 12 - iter 155/319 - loss 0.85788610\n",
      "2019-06-18 09:18:42,145 epoch 12 - iter 186/319 - loss 0.84381055\n",
      "2019-06-18 09:19:32,042 epoch 12 - iter 217/319 - loss 0.85548664\n",
      "2019-06-18 09:20:17,659 epoch 12 - iter 248/319 - loss 0.86251563\n",
      "2019-06-18 09:21:12,419 epoch 12 - iter 279/319 - loss 0.86268496\n",
      "2019-06-18 09:22:04,965 epoch 12 - iter 310/319 - loss 0.86185759\n",
      "2019-06-18 09:22:16,003 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 09:22:16,007 EPOCH 12 done: loss 0.8608 - lr 0.1000 - bad epochs 2\n",
      "2019-06-18 09:24:41,324 DEV : loss 1.1402153968811035 - score 0.6091\n",
      "2019-06-18 09:26:40,667 TEST : loss 1.1709774732589722 - score 0.6172\n",
      "2019-06-18 09:26:46,045 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 09:26:47,474 epoch 13 - iter 0/319 - loss 0.17051271\n",
      "2019-06-18 09:27:51,430 epoch 13 - iter 31/319 - loss 0.79424806\n",
      "2019-06-18 09:28:37,736 epoch 13 - iter 62/319 - loss 0.79235868\n",
      "2019-06-18 09:29:24,744 epoch 13 - iter 93/319 - loss 0.78483182\n",
      "2019-06-18 09:30:13,474 epoch 13 - iter 124/319 - loss 0.78543675\n",
      "2019-06-18 09:30:59,524 epoch 13 - iter 155/319 - loss 0.79256120\n",
      "2019-06-18 09:32:01,711 epoch 13 - iter 186/319 - loss 0.81330235\n",
      "2019-06-18 09:32:54,133 epoch 13 - iter 217/319 - loss 0.82787033\n",
      "2019-06-18 09:33:34,305 epoch 13 - iter 248/319 - loss 0.82378553\n",
      "2019-06-18 09:34:30,321 epoch 13 - iter 279/319 - loss 0.82788363\n",
      "2019-06-18 09:35:16,790 epoch 13 - iter 310/319 - loss 0.83495782\n",
      "2019-06-18 09:35:26,438 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 09:35:26,440 EPOCH 13 done: loss 0.8257 - lr 0.1000 - bad epochs 3\n",
      "2019-06-18 09:37:52,062 DEV : loss 1.0886269807815552 - score 0.6129\n",
      "2019-06-18 09:39:50,779 TEST : loss 1.1506346464157104 - score 0.601\n",
      "Epoch    12: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2019-06-18 09:39:56,231 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 09:39:58,694 epoch 14 - iter 0/319 - loss 1.29366815\n",
      "2019-06-18 09:40:45,675 epoch 14 - iter 31/319 - loss 0.90800683\n",
      "2019-06-18 09:41:28,681 epoch 14 - iter 62/319 - loss 0.80589686\n",
      "2019-06-18 09:42:45,576 epoch 14 - iter 93/319 - loss 0.82068096\n",
      "2019-06-18 09:43:30,292 epoch 14 - iter 124/319 - loss 0.82379701\n",
      "2019-06-18 09:44:12,662 epoch 14 - iter 155/319 - loss 0.79185109\n",
      "2019-06-18 09:45:00,092 epoch 14 - iter 186/319 - loss 0.78395755\n",
      "2019-06-18 09:45:59,578 epoch 14 - iter 217/319 - loss 0.79729052\n",
      "2019-06-18 09:45:59,578 epoch 14 - iter 217/319 - loss 0.79729052\n",
      "2019-06-18 09:46:48,279 epoch 14 - iter 248/319 - loss 0.77879663\n",
      "2019-06-18 09:46:48,279 epoch 14 - iter 248/319 - loss 0.77879663\n",
      "2019-06-18 09:47:32,833 epoch 14 - iter 279/319 - loss 0.77226613\n",
      "2019-06-18 09:47:32,833 epoch 14 - iter 279/319 - loss 0.77226613\n",
      "2019-06-18 09:48:15,865 epoch 14 - iter 310/319 - loss 0.76762970\n",
      "2019-06-18 09:48:15,865 epoch 14 - iter 310/319 - loss 0.76762970\n",
      "2019-06-18 09:48:30,025 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 09:48:30,025 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 09:48:30,031 EPOCH 14 done: loss 0.7652 - lr 0.0500 - bad epochs 0\n",
      "2019-06-18 09:48:30,031 EPOCH 14 done: loss 0.7652 - lr 0.0500 - bad epochs 0\n",
      "2019-06-18 09:50:56,633 DEV : loss 1.054129719734192 - score 0.6288\n",
      "2019-06-18 09:50:56,633 DEV : loss 1.054129719734192 - score 0.6288\n",
      "2019-06-18 09:52:53,911 TEST : loss 1.0963735580444336 - score 0.6263\n",
      "2019-06-18 09:52:53,911 TEST : loss 1.0963735580444336 - score 0.6263\n",
      "2019-06-18 09:53:04,512 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 09:53:04,512 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 09:53:13,884 epoch 15 - iter 0/319 - loss 0.36132860\n",
      "2019-06-18 09:53:13,884 epoch 15 - iter 0/319 - loss 0.36132860\n",
      "2019-06-18 09:54:00,214 epoch 15 - iter 31/319 - loss 0.71666859\n",
      "2019-06-18 09:54:00,214 epoch 15 - iter 31/319 - loss 0.71666859\n",
      "2019-06-18 09:55:01,168 epoch 15 - iter 62/319 - loss 0.68935023\n",
      "2019-06-18 09:55:01,168 epoch 15 - iter 62/319 - loss 0.68935023\n",
      "2019-06-18 09:55:58,676 epoch 15 - iter 93/319 - loss 0.70105800\n",
      "2019-06-18 09:55:58,676 epoch 15 - iter 93/319 - loss 0.70105800\n",
      "2019-06-18 09:56:41,452 epoch 15 - iter 124/319 - loss 0.76028621\n",
      "2019-06-18 09:56:41,452 epoch 15 - iter 124/319 - loss 0.76028621\n",
      "2019-06-18 09:57:27,620 epoch 15 - iter 155/319 - loss 0.74071265\n",
      "2019-06-18 09:57:27,620 epoch 15 - iter 155/319 - loss 0.74071265\n",
      "2019-06-18 09:58:23,343 epoch 15 - iter 186/319 - loss 0.73244125\n",
      "2019-06-18 09:58:23,343 epoch 15 - iter 186/319 - loss 0.73244125\n",
      "2019-06-18 09:59:09,404 epoch 15 - iter 217/319 - loss 0.71850942\n",
      "2019-06-18 09:59:09,404 epoch 15 - iter 217/319 - loss 0.71850942\n",
      "2019-06-18 09:59:56,432 epoch 15 - iter 248/319 - loss 0.72093995\n",
      "2019-06-18 09:59:56,432 epoch 15 - iter 248/319 - loss 0.72093995\n",
      "2019-06-18 10:00:40,680 epoch 15 - iter 279/319 - loss 0.72435372\n",
      "2019-06-18 10:00:40,680 epoch 15 - iter 279/319 - loss 0.72435372\n",
      "2019-06-18 10:01:40,753 epoch 15 - iter 310/319 - loss 0.72189017\n",
      "2019-06-18 10:01:40,753 epoch 15 - iter 310/319 - loss 0.72189017\n",
      "2019-06-18 10:01:49,617 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 10:01:49,617 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 10:01:49,619 EPOCH 15 done: loss 0.7183 - lr 0.0500 - bad epochs 0\n",
      "2019-06-18 10:01:49,619 EPOCH 15 done: loss 0.7183 - lr 0.0500 - bad epochs 0\n",
      "2019-06-18 10:04:16,847 DEV : loss 1.1025969982147217 - score 0.6203\n",
      "2019-06-18 10:04:16,847 DEV : loss 1.1025969982147217 - score 0.6203\n",
      "2019-06-18 10:06:13,925 TEST : loss 1.1510443687438965 - score 0.618\n",
      "2019-06-18 10:06:13,925 TEST : loss 1.1510443687438965 - score 0.618\n",
      "2019-06-18 10:06:19,248 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 10:06:19,248 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 10:06:21,055 epoch 16 - iter 0/319 - loss 0.21252513\n",
      "2019-06-18 10:06:21,055 epoch 16 - iter 0/319 - loss 0.21252513\n",
      "2019-06-18 10:07:15,829 epoch 16 - iter 31/319 - loss 0.72925331\n",
      "2019-06-18 10:07:15,829 epoch 16 - iter 31/319 - loss 0.72925331\n",
      "2019-06-18 10:08:00,147 epoch 16 - iter 62/319 - loss 0.73135850\n",
      "2019-06-18 10:08:00,147 epoch 16 - iter 62/319 - loss 0.73135850\n",
      "2019-06-18 10:08:52,364 epoch 16 - iter 93/319 - loss 0.73325042\n",
      "2019-06-18 10:08:52,364 epoch 16 - iter 93/319 - loss 0.73325042\n",
      "2019-06-18 10:09:33,353 epoch 16 - iter 124/319 - loss 0.73426258\n",
      "2019-06-18 10:09:33,353 epoch 16 - iter 124/319 - loss 0.73426258\n",
      "2019-06-18 10:10:16,467 epoch 16 - iter 155/319 - loss 0.68930502\n",
      "2019-06-18 10:10:16,467 epoch 16 - iter 155/319 - loss 0.68930502\n",
      "2019-06-18 10:11:09,362 epoch 16 - iter 186/319 - loss 0.69782071\n",
      "2019-06-18 10:11:09,362 epoch 16 - iter 186/319 - loss 0.69782071\n",
      "2019-06-18 10:12:03,531 epoch 16 - iter 217/319 - loss 0.69356169\n",
      "2019-06-18 10:12:03,531 epoch 16 - iter 217/319 - loss 0.69356169\n",
      "2019-06-18 10:12:57,963 epoch 16 - iter 248/319 - loss 0.69549815\n",
      "2019-06-18 10:12:57,963 epoch 16 - iter 248/319 - loss 0.69549815\n",
      "2019-06-18 10:13:59,313 epoch 16 - iter 279/319 - loss 0.70643182\n",
      "2019-06-18 10:13:59,313 epoch 16 - iter 279/319 - loss 0.70643182\n",
      "2019-06-18 10:14:48,285 epoch 16 - iter 310/319 - loss 0.71246680\n",
      "2019-06-18 10:14:48,285 epoch 16 - iter 310/319 - loss 0.71246680\n",
      "2019-06-18 10:14:57,511 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 10:14:57,511 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 10:14:57,512 EPOCH 16 done: loss 0.7122 - lr 0.0500 - bad epochs 1\n",
      "2019-06-18 10:14:57,512 EPOCH 16 done: loss 0.7122 - lr 0.0500 - bad epochs 1\n",
      "2019-06-18 10:17:24,914 DEV : loss 1.075132966041565 - score 0.6053\n",
      "2019-06-18 10:17:24,914 DEV : loss 1.075132966041565 - score 0.6053\n",
      "2019-06-18 10:19:22,663 TEST : loss 1.125848412513733 - score 0.6092\n",
      "2019-06-18 10:19:22,663 TEST : loss 1.125848412513733 - score 0.6092\n",
      "2019-06-18 10:19:28,023 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 10:19:28,023 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 10:19:29,454 epoch 17 - iter 0/319 - loss 0.57525557\n",
      "2019-06-18 10:19:29,454 epoch 17 - iter 0/319 - loss 0.57525557\n",
      "2019-06-18 10:20:14,256 epoch 17 - iter 31/319 - loss 0.67738022\n",
      "2019-06-18 10:20:14,256 epoch 17 - iter 31/319 - loss 0.67738022\n",
      "2019-06-18 10:20:58,063 epoch 17 - iter 62/319 - loss 0.67554830\n",
      "2019-06-18 10:20:58,063 epoch 17 - iter 62/319 - loss 0.67554830\n",
      "2019-06-18 10:22:01,430 epoch 17 - iter 93/319 - loss 0.67922997\n",
      "2019-06-18 10:22:01,430 epoch 17 - iter 93/319 - loss 0.67922997\n",
      "2019-06-18 10:22:46,985 epoch 17 - iter 124/319 - loss 0.67993236\n",
      "2019-06-18 10:22:46,985 epoch 17 - iter 124/319 - loss 0.67993236\n",
      "2019-06-18 10:23:27,785 epoch 17 - iter 155/319 - loss 0.68618548\n",
      "2019-06-18 10:23:27,785 epoch 17 - iter 155/319 - loss 0.68618548\n",
      "2019-06-18 10:24:10,064 epoch 17 - iter 186/319 - loss 0.67415550\n",
      "2019-06-18 10:24:10,064 epoch 17 - iter 186/319 - loss 0.67415550\n",
      "2019-06-18 10:25:07,607 epoch 17 - iter 217/319 - loss 0.68378486\n",
      "2019-06-18 10:25:07,607 epoch 17 - iter 217/319 - loss 0.68378486\n",
      "2019-06-18 10:26:00,278 epoch 17 - iter 248/319 - loss 0.68716489\n",
      "2019-06-18 10:26:00,278 epoch 17 - iter 248/319 - loss 0.68716489\n",
      "2019-06-18 10:26:53,187 epoch 17 - iter 279/319 - loss 0.69160568\n",
      "2019-06-18 10:26:53,187 epoch 17 - iter 279/319 - loss 0.69160568\n",
      "2019-06-18 10:27:58,551 epoch 17 - iter 310/319 - loss 0.68855409\n",
      "2019-06-18 10:27:58,551 epoch 17 - iter 310/319 - loss 0.68855409\n",
      "2019-06-18 10:28:07,596 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 10:28:07,596 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 10:28:07,597 EPOCH 17 done: loss 0.6875 - lr 0.0500 - bad epochs 2\n",
      "2019-06-18 10:28:07,597 EPOCH 17 done: loss 0.6875 - lr 0.0500 - bad epochs 2\n",
      "2019-06-18 10:30:34,839 DEV : loss 1.11024010181427 - score 0.6206\n",
      "2019-06-18 10:30:34,839 DEV : loss 1.11024010181427 - score 0.6206\n",
      "2019-06-18 10:32:32,425 TEST : loss 1.1508123874664307 - score 0.6286\n",
      "2019-06-18 10:32:32,425 TEST : loss 1.1508123874664307 - score 0.6286\n",
      "2019-06-18 10:32:37,839 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 10:32:37,839 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 10:32:39,977 epoch 18 - iter 0/319 - loss 1.03838468\n",
      "2019-06-18 10:32:39,977 epoch 18 - iter 0/319 - loss 1.03838468\n",
      "2019-06-18 10:33:19,069 epoch 18 - iter 31/319 - loss 0.69261051\n",
      "2019-06-18 10:33:19,069 epoch 18 - iter 31/319 - loss 0.69261051\n",
      "2019-06-18 10:34:09,671 epoch 18 - iter 62/319 - loss 0.69747888\n",
      "2019-06-18 10:34:09,671 epoch 18 - iter 62/319 - loss 0.69747888\n",
      "2019-06-18 10:34:50,632 epoch 18 - iter 93/319 - loss 0.67508344\n",
      "2019-06-18 10:34:50,632 epoch 18 - iter 93/319 - loss 0.67508344\n",
      "2019-06-18 10:35:30,605 epoch 18 - iter 124/319 - loss 0.66573591\n",
      "2019-06-18 10:35:30,605 epoch 18 - iter 124/319 - loss 0.66573591\n",
      "2019-06-18 10:36:25,024 epoch 18 - iter 155/319 - loss 0.66358534\n",
      "2019-06-18 10:36:25,024 epoch 18 - iter 155/319 - loss 0.66358534\n",
      "2019-06-18 10:37:12,376 epoch 18 - iter 186/319 - loss 0.67276012\n",
      "2019-06-18 10:37:12,376 epoch 18 - iter 186/319 - loss 0.67276012\n",
      "2019-06-18 10:37:55,813 epoch 18 - iter 217/319 - loss 0.67369688\n",
      "2019-06-18 10:37:55,813 epoch 18 - iter 217/319 - loss 0.67369688\n",
      "2019-06-18 10:39:29,377 epoch 18 - iter 248/319 - loss 0.67375379\n",
      "2019-06-18 10:39:29,377 epoch 18 - iter 248/319 - loss 0.67375379\n",
      "2019-06-18 10:40:26,140 epoch 18 - iter 279/319 - loss 0.67576253\n",
      "2019-06-18 10:40:26,140 epoch 18 - iter 279/319 - loss 0.67576253\n",
      "2019-06-18 10:41:15,252 epoch 18 - iter 310/319 - loss 0.68053731\n",
      "2019-06-18 10:41:15,252 epoch 18 - iter 310/319 - loss 0.68053731\n",
      "2019-06-18 10:41:27,532 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 10:41:27,532 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 10:41:27,534 EPOCH 18 done: loss 0.6838 - lr 0.0500 - bad epochs 3\n",
      "2019-06-18 10:41:27,534 EPOCH 18 done: loss 0.6838 - lr 0.0500 - bad epochs 3\n",
      "2019-06-18 10:43:59,822 DEV : loss 1.0785552263259888 - score 0.6138\n",
      "2019-06-18 10:43:59,822 DEV : loss 1.0785552263259888 - score 0.6138\n",
      "2019-06-18 10:46:01,935 TEST : loss 1.144274353981018 - score 0.6142\n",
      "2019-06-18 10:46:01,935 TEST : loss 1.144274353981018 - score 0.6142\n",
      "Epoch    17: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Epoch    17: reducing learning rate of group 0 to 2.5000e-02.\n",
      "2019-06-18 10:46:07,282 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 10:46:07,282 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 10:46:08,683 epoch 19 - iter 0/319 - loss 0.55861855\n",
      "2019-06-18 10:46:08,683 epoch 19 - iter 0/319 - loss 0.55861855\n",
      "2019-06-18 10:47:12,354 epoch 19 - iter 31/319 - loss 0.62149540\n",
      "2019-06-18 10:47:12,354 epoch 19 - iter 31/319 - loss 0.62149540\n",
      "2019-06-18 10:48:00,392 epoch 19 - iter 62/319 - loss 0.67007146\n",
      "2019-06-18 10:48:00,392 epoch 19 - iter 62/319 - loss 0.67007146\n",
      "2019-06-18 10:48:41,503 epoch 19 - iter 93/319 - loss 0.63294166\n",
      "2019-06-18 10:48:41,503 epoch 19 - iter 93/319 - loss 0.63294166\n",
      "2019-06-18 10:49:26,316 epoch 19 - iter 124/319 - loss 0.61755308\n",
      "2019-06-18 10:49:26,316 epoch 19 - iter 124/319 - loss 0.61755308\n",
      "2019-06-18 10:50:04,777 epoch 19 - iter 155/319 - loss 0.61347681\n",
      "2019-06-18 10:50:04,777 epoch 19 - iter 155/319 - loss 0.61347681\n",
      "2019-06-18 10:51:18,939 epoch 19 - iter 186/319 - loss 0.63270886\n",
      "2019-06-18 10:51:18,939 epoch 19 - iter 186/319 - loss 0.63270886\n",
      "2019-06-18 10:52:12,546 epoch 19 - iter 217/319 - loss 0.63574420\n",
      "2019-06-18 10:52:12,546 epoch 19 - iter 217/319 - loss 0.63574420\n",
      "2019-06-18 10:53:06,231 epoch 19 - iter 248/319 - loss 0.64256085\n",
      "2019-06-18 10:53:06,231 epoch 19 - iter 248/319 - loss 0.64256085\n",
      "2019-06-18 10:53:48,431 epoch 19 - iter 279/319 - loss 0.63786729\n",
      "2019-06-18 10:53:48,431 epoch 19 - iter 279/319 - loss 0.63786729\n",
      "2019-06-18 10:54:44,158 epoch 19 - iter 310/319 - loss 0.63852255\n",
      "2019-06-18 10:54:44,158 epoch 19 - iter 310/319 - loss 0.63852255\n",
      "2019-06-18 10:54:59,309 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 10:54:59,309 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 10:54:59,310 EPOCH 19 done: loss 0.6392 - lr 0.0250 - bad epochs 0\n",
      "2019-06-18 10:54:59,310 EPOCH 19 done: loss 0.6392 - lr 0.0250 - bad epochs 0\n",
      "2019-06-18 10:57:29,731 DEV : loss 1.0726605653762817 - score 0.6286\n",
      "2019-06-18 10:57:29,731 DEV : loss 1.0726605653762817 - score 0.6286\n",
      "2019-06-18 10:59:31,414 TEST : loss 1.1443486213684082 - score 0.6232\n",
      "2019-06-18 10:59:31,414 TEST : loss 1.1443486213684082 - score 0.6232\n",
      "2019-06-18 10:59:36,611 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 10:59:36,611 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 10:59:38,490 epoch 20 - iter 0/319 - loss 0.46567285\n",
      "2019-06-18 10:59:38,490 epoch 20 - iter 0/319 - loss 0.46567285\n",
      "2019-06-18 11:00:51,957 epoch 20 - iter 31/319 - loss 0.73618732\n",
      "2019-06-18 11:00:51,957 epoch 20 - iter 31/319 - loss 0.73618732\n",
      "2019-06-18 11:01:30,867 epoch 20 - iter 62/319 - loss 0.63542242\n",
      "2019-06-18 11:01:30,867 epoch 20 - iter 62/319 - loss 0.63542242\n",
      "2019-06-18 11:02:26,283 epoch 20 - iter 93/319 - loss 0.62644609\n",
      "2019-06-18 11:02:26,283 epoch 20 - iter 93/319 - loss 0.62644609\n",
      "2019-06-18 11:03:10,287 epoch 20 - iter 124/319 - loss 0.63522112\n",
      "2019-06-18 11:03:10,287 epoch 20 - iter 124/319 - loss 0.63522112\n",
      "2019-06-18 11:03:55,390 epoch 20 - iter 155/319 - loss 0.61204427\n",
      "2019-06-18 11:03:55,390 epoch 20 - iter 155/319 - loss 0.61204427\n",
      "2019-06-18 11:04:37,075 epoch 20 - iter 186/319 - loss 0.61211473\n",
      "2019-06-18 11:04:37,075 epoch 20 - iter 186/319 - loss 0.61211473\n",
      "2019-06-18 11:05:53,630 epoch 20 - iter 217/319 - loss 0.61226911\n",
      "2019-06-18 11:05:53,630 epoch 20 - iter 217/319 - loss 0.61226911\n",
      "2019-06-18 11:06:45,877 epoch 20 - iter 248/319 - loss 0.61803879\n",
      "2019-06-18 11:06:45,877 epoch 20 - iter 248/319 - loss 0.61803879\n",
      "2019-06-18 11:07:33,667 epoch 20 - iter 279/319 - loss 0.61345745\n",
      "2019-06-18 11:07:33,667 epoch 20 - iter 279/319 - loss 0.61345745\n",
      "2019-06-18 11:08:20,490 epoch 20 - iter 310/319 - loss 0.61409652\n",
      "2019-06-18 11:08:20,490 epoch 20 - iter 310/319 - loss 0.61409652\n",
      "2019-06-18 11:08:34,546 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 11:08:34,546 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 11:08:34,548 EPOCH 20 done: loss 0.6138 - lr 0.0250 - bad epochs 1\n",
      "2019-06-18 11:08:34,548 EPOCH 20 done: loss 0.6138 - lr 0.0250 - bad epochs 1\n",
      "2019-06-18 11:11:04,960 DEV : loss 1.0785725116729736 - score 0.6197\n",
      "2019-06-18 11:11:04,960 DEV : loss 1.0785725116729736 - score 0.6197\n",
      "2019-06-18 11:13:06,688 TEST : loss 1.149461030960083 - score 0.6213\n",
      "2019-06-18 11:13:06,688 TEST : loss 1.149461030960083 - score 0.6213\n",
      "2019-06-18 11:13:11,879 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 11:13:11,879 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 11:13:14,472 epoch 21 - iter 0/319 - loss 0.23164003\n",
      "2019-06-18 11:13:14,472 epoch 21 - iter 0/319 - loss 0.23164003\n",
      "2019-06-18 11:14:00,252 epoch 21 - iter 31/319 - loss 0.59393482\n",
      "2019-06-18 11:14:00,252 epoch 21 - iter 31/319 - loss 0.59393482\n",
      "2019-06-18 11:14:53,101 epoch 21 - iter 62/319 - loss 0.57189997\n",
      "2019-06-18 11:14:53,101 epoch 21 - iter 62/319 - loss 0.57189997\n",
      "2019-06-18 11:15:55,602 epoch 21 - iter 93/319 - loss 0.59691464\n",
      "2019-06-18 11:15:55,602 epoch 21 - iter 93/319 - loss 0.59691464\n",
      "2019-06-18 11:16:36,936 epoch 21 - iter 124/319 - loss 0.59436513\n",
      "2019-06-18 11:16:36,936 epoch 21 - iter 124/319 - loss 0.59436513\n",
      "2019-06-18 11:17:32,225 epoch 21 - iter 155/319 - loss 0.61729857\n",
      "2019-06-18 11:17:32,225 epoch 21 - iter 155/319 - loss 0.61729857\n",
      "2019-06-18 11:18:13,900 epoch 21 - iter 186/319 - loss 0.62551702\n",
      "2019-06-18 11:18:13,900 epoch 21 - iter 186/319 - loss 0.62551702\n",
      "2019-06-18 11:18:55,000 epoch 21 - iter 217/319 - loss 0.62120608\n",
      "2019-06-18 11:18:55,000 epoch 21 - iter 217/319 - loss 0.62120608\n",
      "2019-06-18 11:19:49,704 epoch 21 - iter 248/319 - loss 0.61645026\n",
      "2019-06-18 11:19:49,704 epoch 21 - iter 248/319 - loss 0.61645026\n",
      "2019-06-18 11:20:34,340 epoch 21 - iter 279/319 - loss 0.61784941\n",
      "2019-06-18 11:20:34,340 epoch 21 - iter 279/319 - loss 0.61784941\n",
      "2019-06-18 11:21:53,541 epoch 21 - iter 310/319 - loss 0.61967066\n",
      "2019-06-18 11:21:53,541 epoch 21 - iter 310/319 - loss 0.61967066\n",
      "2019-06-18 11:22:07,781 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 11:22:07,781 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 11:22:07,783 EPOCH 21 done: loss 0.6182 - lr 0.0250 - bad epochs 2\n",
      "2019-06-18 11:22:07,783 EPOCH 21 done: loss 0.6182 - lr 0.0250 - bad epochs 2\n",
      "2019-06-18 11:24:37,848 DEV : loss 1.0846588611602783 - score 0.6152\n",
      "2019-06-18 11:24:37,848 DEV : loss 1.0846588611602783 - score 0.6152\n",
      "2019-06-18 11:26:39,585 TEST : loss 1.157579779624939 - score 0.6217\n",
      "2019-06-18 11:26:39,585 TEST : loss 1.157579779624939 - score 0.6217\n",
      "2019-06-18 11:26:44,929 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 11:26:44,929 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 11:26:46,716 epoch 22 - iter 0/319 - loss 0.20645809\n",
      "2019-06-18 11:26:46,716 epoch 22 - iter 0/319 - loss 0.20645809\n",
      "2019-06-18 11:27:32,416 epoch 22 - iter 31/319 - loss 0.54355063\n",
      "2019-06-18 11:27:32,416 epoch 22 - iter 31/319 - loss 0.54355063\n",
      "2019-06-18 11:28:23,116 epoch 22 - iter 62/319 - loss 0.59179141\n",
      "2019-06-18 11:28:23,116 epoch 22 - iter 62/319 - loss 0.59179141\n",
      "2019-06-18 11:29:13,239 epoch 22 - iter 93/319 - loss 0.57132444\n",
      "2019-06-18 11:29:13,239 epoch 22 - iter 93/319 - loss 0.57132444\n",
      "2019-06-18 11:30:29,111 epoch 22 - iter 124/319 - loss 0.57853917\n",
      "2019-06-18 11:30:29,111 epoch 22 - iter 124/319 - loss 0.57853917\n",
      "2019-06-18 11:31:36,613 epoch 22 - iter 155/319 - loss 0.59974955\n",
      "2019-06-18 11:31:36,613 epoch 22 - iter 155/319 - loss 0.59974955\n",
      "2019-06-18 11:32:35,118 epoch 22 - iter 186/319 - loss 0.61467375\n",
      "2019-06-18 11:32:35,118 epoch 22 - iter 186/319 - loss 0.61467375\n",
      "2019-06-18 11:33:16,109 epoch 22 - iter 217/319 - loss 0.61843892\n",
      "2019-06-18 11:33:16,109 epoch 22 - iter 217/319 - loss 0.61843892\n",
      "2019-06-18 11:34:10,255 epoch 22 - iter 248/319 - loss 0.61027654\n",
      "2019-06-18 11:34:10,255 epoch 22 - iter 248/319 - loss 0.61027654\n",
      "2019-06-18 11:34:47,014 epoch 22 - iter 279/319 - loss 0.60618326\n",
      "2019-06-18 11:34:47,014 epoch 22 - iter 279/319 - loss 0.60618326\n",
      "2019-06-18 11:35:34,634 epoch 22 - iter 310/319 - loss 0.60871595\n",
      "2019-06-18 11:35:34,634 epoch 22 - iter 310/319 - loss 0.60871595\n",
      "2019-06-18 11:35:44,344 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 11:35:44,344 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 11:35:44,347 EPOCH 22 done: loss 0.6039 - lr 0.0250 - bad epochs 3\n",
      "2019-06-18 11:35:44,347 EPOCH 22 done: loss 0.6039 - lr 0.0250 - bad epochs 3\n",
      "2019-06-18 11:38:14,822 DEV : loss 1.0944682359695435 - score 0.6203\n",
      "2019-06-18 11:38:14,822 DEV : loss 1.0944682359695435 - score 0.6203\n",
      "2019-06-18 11:40:16,616 TEST : loss 1.1562484502792358 - score 0.623\n",
      "2019-06-18 11:40:16,616 TEST : loss 1.1562484502792358 - score 0.623\n",
      "Epoch    21: reducing learning rate of group 0 to 1.2500e-02.\n",
      "Epoch    21: reducing learning rate of group 0 to 1.2500e-02.\n",
      "2019-06-18 11:40:21,935 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 11:40:21,935 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 11:40:23,995 epoch 23 - iter 0/319 - loss 0.52874899\n",
      "2019-06-18 11:40:23,995 epoch 23 - iter 0/319 - loss 0.52874899\n",
      "2019-06-18 11:41:14,229 epoch 23 - iter 31/319 - loss 0.64436188\n",
      "2019-06-18 11:41:14,229 epoch 23 - iter 31/319 - loss 0.64436188\n",
      "2019-06-18 11:42:14,758 epoch 23 - iter 62/319 - loss 0.61475234\n",
      "2019-06-18 11:42:14,758 epoch 23 - iter 62/319 - loss 0.61475234\n",
      "2019-06-18 11:43:29,345 epoch 23 - iter 93/319 - loss 0.60442546\n",
      "2019-06-18 11:43:29,345 epoch 23 - iter 93/319 - loss 0.60442546\n",
      "2019-06-18 11:44:14,668 epoch 23 - iter 124/319 - loss 0.61215155\n",
      "2019-06-18 11:44:14,668 epoch 23 - iter 124/319 - loss 0.61215155\n",
      "2019-06-18 11:44:58,764 epoch 23 - iter 155/319 - loss 0.58808696\n",
      "2019-06-18 11:44:58,764 epoch 23 - iter 155/319 - loss 0.58808696\n",
      "2019-06-18 11:45:52,441 epoch 23 - iter 186/319 - loss 0.59263062\n",
      "2019-06-18 11:45:52,441 epoch 23 - iter 186/319 - loss 0.59263062\n",
      "2019-06-18 11:46:51,386 epoch 23 - iter 217/319 - loss 0.59468401\n",
      "2019-06-18 11:46:51,386 epoch 23 - iter 217/319 - loss 0.59468401\n",
      "2019-06-18 11:47:35,115 epoch 23 - iter 248/319 - loss 0.59223003\n",
      "2019-06-18 11:47:35,115 epoch 23 - iter 248/319 - loss 0.59223003\n",
      "2019-06-18 11:48:17,955 epoch 23 - iter 279/319 - loss 0.58427222\n",
      "2019-06-18 11:48:17,955 epoch 23 - iter 279/319 - loss 0.58427222\n",
      "2019-06-18 11:49:16,166 epoch 23 - iter 310/319 - loss 0.58245828\n",
      "2019-06-18 11:49:16,166 epoch 23 - iter 310/319 - loss 0.58245828\n",
      "2019-06-18 11:49:25,860 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 11:49:25,860 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 11:49:25,862 EPOCH 23 done: loss 0.5851 - lr 0.0125 - bad epochs 0\n",
      "2019-06-18 11:49:25,862 EPOCH 23 done: loss 0.5851 - lr 0.0125 - bad epochs 0\n",
      "2019-06-18 11:51:56,263 DEV : loss 1.0742919445037842 - score 0.6272\n",
      "2019-06-18 11:51:56,263 DEV : loss 1.0742919445037842 - score 0.6272\n",
      "2019-06-18 11:53:58,171 TEST : loss 1.1537261009216309 - score 0.6187\n",
      "2019-06-18 11:53:58,171 TEST : loss 1.1537261009216309 - score 0.6187\n",
      "2019-06-18 11:54:03,410 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 11:54:03,410 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 11:54:05,625 epoch 24 - iter 0/319 - loss 0.61761022\n",
      "2019-06-18 11:54:05,625 epoch 24 - iter 0/319 - loss 0.61761022\n",
      "2019-06-18 11:55:02,276 epoch 24 - iter 31/319 - loss 0.55167130\n",
      "2019-06-18 11:55:02,276 epoch 24 - iter 31/319 - loss 0.55167130\n",
      "2019-06-18 11:55:56,458 epoch 24 - iter 62/319 - loss 0.59062755\n",
      "2019-06-18 11:55:56,458 epoch 24 - iter 62/319 - loss 0.59062755\n",
      "2019-06-18 11:56:52,077 epoch 24 - iter 93/319 - loss 0.57551537\n",
      "2019-06-18 11:56:52,077 epoch 24 - iter 93/319 - loss 0.57551537\n",
      "2019-06-18 11:57:39,810 epoch 24 - iter 124/319 - loss 0.57189096\n",
      "2019-06-18 11:57:39,810 epoch 24 - iter 124/319 - loss 0.57189096\n",
      "2019-06-18 11:58:16,555 epoch 24 - iter 155/319 - loss 0.56946230\n",
      "2019-06-18 11:58:16,555 epoch 24 - iter 155/319 - loss 0.56946230\n",
      "2019-06-18 11:59:09,908 epoch 24 - iter 186/319 - loss 0.56352113\n",
      "2019-06-18 11:59:09,908 epoch 24 - iter 186/319 - loss 0.56352113\n",
      "2019-06-18 12:00:18,760 epoch 24 - iter 217/319 - loss 0.56158149\n",
      "2019-06-18 12:00:18,760 epoch 24 - iter 217/319 - loss 0.56158149\n",
      "2019-06-18 12:01:11,839 epoch 24 - iter 248/319 - loss 0.56056482\n",
      "2019-06-18 12:01:11,839 epoch 24 - iter 248/319 - loss 0.56056482\n",
      "2019-06-18 12:02:03,642 epoch 24 - iter 279/319 - loss 0.55260409\n",
      "2019-06-18 12:02:03,642 epoch 24 - iter 279/319 - loss 0.55260409\n",
      "2019-06-18 12:02:46,678 epoch 24 - iter 310/319 - loss 0.55527752\n",
      "2019-06-18 12:02:46,678 epoch 24 - iter 310/319 - loss 0.55527752\n",
      "2019-06-18 12:03:03,684 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 12:03:03,684 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 12:03:03,686 EPOCH 24 done: loss 0.5638 - lr 0.0125 - bad epochs 1\n",
      "2019-06-18 12:03:03,686 EPOCH 24 done: loss 0.5638 - lr 0.0125 - bad epochs 1\n",
      "2019-06-18 12:05:34,089 DEV : loss 1.0767951011657715 - score 0.6275\n",
      "2019-06-18 12:05:34,089 DEV : loss 1.0767951011657715 - score 0.6275\n",
      "2019-06-18 12:07:35,848 TEST : loss 1.1444271802902222 - score 0.6197\n",
      "2019-06-18 12:07:35,848 TEST : loss 1.1444271802902222 - score 0.6197\n",
      "2019-06-18 12:07:41,250 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 12:07:41,250 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 12:07:43,669 epoch 25 - iter 0/319 - loss 0.71385163\n",
      "2019-06-18 12:07:43,669 epoch 25 - iter 0/319 - loss 0.71385163\n",
      "2019-06-18 12:08:25,544 epoch 25 - iter 31/319 - loss 0.61276001\n",
      "2019-06-18 12:08:25,544 epoch 25 - iter 31/319 - loss 0.61276001\n",
      "2019-06-18 12:09:18,113 epoch 25 - iter 62/319 - loss 0.60203260\n",
      "2019-06-18 12:09:18,113 epoch 25 - iter 62/319 - loss 0.60203260\n",
      "2019-06-18 12:10:26,306 epoch 25 - iter 93/319 - loss 0.64286984\n",
      "2019-06-18 12:10:26,306 epoch 25 - iter 93/319 - loss 0.64286984\n",
      "2019-06-18 12:11:28,751 epoch 25 - iter 124/319 - loss 0.60557325\n",
      "2019-06-18 12:11:28,751 epoch 25 - iter 124/319 - loss 0.60557325\n",
      "2019-06-18 12:12:17,225 epoch 25 - iter 155/319 - loss 0.60196731\n",
      "2019-06-18 12:12:17,225 epoch 25 - iter 155/319 - loss 0.60196731\n",
      "2019-06-18 12:12:58,037 epoch 25 - iter 186/319 - loss 0.58171947\n",
      "2019-06-18 12:12:58,037 epoch 25 - iter 186/319 - loss 0.58171947\n",
      "2019-06-18 12:13:52,777 epoch 25 - iter 217/319 - loss 0.57436692\n",
      "2019-06-18 12:13:52,777 epoch 25 - iter 217/319 - loss 0.57436692\n",
      "2019-06-18 12:14:28,003 epoch 25 - iter 248/319 - loss 0.56540509\n",
      "2019-06-18 12:14:28,003 epoch 25 - iter 248/319 - loss 0.56540509\n",
      "2019-06-18 12:15:23,408 epoch 25 - iter 279/319 - loss 0.57363195\n",
      "2019-06-18 12:15:23,408 epoch 25 - iter 279/319 - loss 0.57363195\n",
      "2019-06-18 12:16:32,586 epoch 25 - iter 310/319 - loss 0.57272923\n",
      "2019-06-18 12:16:32,586 epoch 25 - iter 310/319 - loss 0.57272923\n",
      "2019-06-18 12:16:43,286 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 12:16:43,286 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 12:16:43,288 EPOCH 25 done: loss 0.5687 - lr 0.0125 - bad epochs 2\n",
      "2019-06-18 12:16:43,288 EPOCH 25 done: loss 0.5687 - lr 0.0125 - bad epochs 2\n",
      "2019-06-18 12:19:13,723 DEV : loss 1.0893418788909912 - score 0.6207\n",
      "2019-06-18 12:19:13,723 DEV : loss 1.0893418788909912 - score 0.6207\n",
      "2019-06-18 12:21:15,204 TEST : loss 1.161941647529602 - score 0.6203\n",
      "2019-06-18 12:21:15,204 TEST : loss 1.161941647529602 - score 0.6203\n",
      "2019-06-18 12:21:20,810 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 12:21:20,810 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 12:21:23,326 epoch 26 - iter 0/319 - loss 0.33064342\n",
      "2019-06-18 12:21:23,326 epoch 26 - iter 0/319 - loss 0.33064342\n",
      "2019-06-18 12:22:35,395 epoch 26 - iter 31/319 - loss 0.57223277\n",
      "2019-06-18 12:22:35,395 epoch 26 - iter 31/319 - loss 0.57223277\n",
      "2019-06-18 12:23:20,694 epoch 26 - iter 62/319 - loss 0.57894723\n",
      "2019-06-18 12:23:20,694 epoch 26 - iter 62/319 - loss 0.57894723\n",
      "2019-06-18 12:24:23,589 epoch 26 - iter 93/319 - loss 0.58240449\n",
      "2019-06-18 12:24:23,589 epoch 26 - iter 93/319 - loss 0.58240449\n",
      "2019-06-18 12:25:30,449 epoch 26 - iter 124/319 - loss 0.57581537\n",
      "2019-06-18 12:25:30,449 epoch 26 - iter 124/319 - loss 0.57581537\n",
      "2019-06-18 12:26:35,982 epoch 26 - iter 155/319 - loss 0.58907012\n",
      "2019-06-18 12:26:35,982 epoch 26 - iter 155/319 - loss 0.58907012\n",
      "2019-06-18 12:27:25,692 epoch 26 - iter 186/319 - loss 0.59922467\n",
      "2019-06-18 12:27:25,692 epoch 26 - iter 186/319 - loss 0.59922467\n",
      "2019-06-18 12:28:06,320 epoch 26 - iter 217/319 - loss 0.58471950\n",
      "2019-06-18 12:28:06,320 epoch 26 - iter 217/319 - loss 0.58471950\n",
      "2019-06-18 12:28:52,808 epoch 26 - iter 248/319 - loss 0.58103758\n",
      "2019-06-18 12:28:52,808 epoch 26 - iter 248/319 - loss 0.58103758\n",
      "2019-06-18 12:29:31,126 epoch 26 - iter 279/319 - loss 0.57670329\n",
      "2019-06-18 12:29:31,126 epoch 26 - iter 279/319 - loss 0.57670329\n",
      "2019-06-18 12:30:08,286 epoch 26 - iter 310/319 - loss 0.56608009\n",
      "2019-06-18 12:30:08,286 epoch 26 - iter 310/319 - loss 0.56608009\n",
      "2019-06-18 12:30:19,675 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 12:30:19,675 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 12:30:19,685 EPOCH 26 done: loss 0.5717 - lr 0.0125 - bad epochs 3\n",
      "2019-06-18 12:30:19,685 EPOCH 26 done: loss 0.5717 - lr 0.0125 - bad epochs 3\n",
      "2019-06-18 12:32:50,922 DEV : loss 1.1048848628997803 - score 0.6214\n",
      "2019-06-18 12:32:50,922 DEV : loss 1.1048848628997803 - score 0.6214\n",
      "2019-06-18 12:34:53,333 TEST : loss 1.1754204034805298 - score 0.6245\n",
      "2019-06-18 12:34:53,333 TEST : loss 1.1754204034805298 - score 0.6245\n",
      "Epoch    25: reducing learning rate of group 0 to 6.2500e-03.\n",
      "Epoch    25: reducing learning rate of group 0 to 6.2500e-03.\n",
      "2019-06-18 12:34:58,820 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 12:34:58,820 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 12:35:00,748 epoch 27 - iter 0/319 - loss 0.53216887\n",
      "2019-06-18 12:35:00,748 epoch 27 - iter 0/319 - loss 0.53216887\n",
      "2019-06-18 12:36:07,881 epoch 27 - iter 31/319 - loss 0.72377591\n",
      "2019-06-18 12:36:07,881 epoch 27 - iter 31/319 - loss 0.72377591\n",
      "2019-06-18 12:36:52,512 epoch 27 - iter 62/319 - loss 0.66472061\n",
      "2019-06-18 12:36:52,512 epoch 27 - iter 62/319 - loss 0.66472061\n",
      "2019-06-18 12:38:06,719 epoch 27 - iter 93/319 - loss 0.64427681\n",
      "2019-06-18 12:38:06,719 epoch 27 - iter 93/319 - loss 0.64427681\n",
      "2019-06-18 12:38:53,240 epoch 27 - iter 124/319 - loss 0.61274907\n",
      "2019-06-18 12:38:53,240 epoch 27 - iter 124/319 - loss 0.61274907\n",
      "2019-06-18 12:39:46,157 epoch 27 - iter 155/319 - loss 0.60461714\n",
      "2019-06-18 12:39:46,157 epoch 27 - iter 155/319 - loss 0.60461714\n",
      "2019-06-18 12:40:52,508 epoch 27 - iter 186/319 - loss 0.60231270\n",
      "2019-06-18 12:40:52,508 epoch 27 - iter 186/319 - loss 0.60231270\n",
      "2019-06-18 12:41:31,978 epoch 27 - iter 217/319 - loss 0.58756581\n",
      "2019-06-18 12:41:31,978 epoch 27 - iter 217/319 - loss 0.58756581\n",
      "2019-06-18 12:42:20,226 epoch 27 - iter 248/319 - loss 0.58345822\n",
      "2019-06-18 12:42:20,226 epoch 27 - iter 248/319 - loss 0.58345822\n",
      "2019-06-18 12:43:07,763 epoch 27 - iter 279/319 - loss 0.58235320\n",
      "2019-06-18 12:43:07,763 epoch 27 - iter 279/319 - loss 0.58235320\n"
     ]
    }
   ],
   "source": [
    "## initialize trainer\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "model_name = 'resources/taggers/resume-ner-1-nd'\n",
    "\n",
    "## start training\n",
    "trainer.train(model_name,\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              #anneal_with_restarts=True,\n",
    "              max_epochs=75\n",
    "             ,checkpoint=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6052
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12719350,
     "status": "ok",
     "timestamp": 1560891905060,
     "user": {
      "displayName": "Armin Roth",
      "photoUrl": "",
      "userId": "08591799635376506733"
     },
     "user_tz": -120
    },
    "id": "nXgZusGjVquz",
    "outputId": "a1175f8e-aa63-45e6-9695-cbef01b26eb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-18 17:27:27,687 loading file resources/taggers/resume-ner-1-nd/checkpoint.pt\n",
      "2019-06-18 17:27:36,446 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 17:27:36,451 Evaluation method: MICRO_F1_SCORE\n",
      "2019-06-18 17:27:37,661 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 17:27:40,500 epoch 34 - iter 0/319 - loss 0.39117417\n",
      "2019-06-18 17:28:20,101 epoch 34 - iter 31/319 - loss 0.54501318\n",
      "2019-06-18 17:29:04,491 epoch 34 - iter 62/319 - loss 0.51542739\n",
      "2019-06-18 17:29:51,333 epoch 34 - iter 93/319 - loss 0.52494561\n",
      "2019-06-18 17:30:37,111 epoch 34 - iter 124/319 - loss 0.52957665\n",
      "2019-06-18 17:31:19,691 epoch 34 - iter 155/319 - loss 0.55303560\n",
      "2019-06-18 17:31:59,491 epoch 34 - iter 186/319 - loss 0.54449522\n",
      "2019-06-18 17:32:59,205 epoch 34 - iter 217/319 - loss 0.54191026\n",
      "2019-06-18 17:34:03,836 epoch 34 - iter 248/319 - loss 0.53935465\n",
      "2019-06-18 17:34:47,202 epoch 34 - iter 279/319 - loss 0.53605325\n",
      "2019-06-18 17:35:56,053 epoch 34 - iter 310/319 - loss 0.54335990\n",
      "2019-06-18 17:36:05,945 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 17:36:05,947 EPOCH 34 done: loss 0.5428 - lr 0.0031 - bad epochs 3\n",
      "2019-06-18 17:38:28,388 DEV : loss 1.0936297178268433 - score 0.6245\n",
      "2019-06-18 17:40:23,858 TEST : loss 1.1655486822128296 - score 0.6218\n",
      "Epoch    33: reducing learning rate of group 0 to 1.5625e-03.\n",
      "2019-06-18 17:40:29,417 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 17:40:32,605 epoch 35 - iter 0/319 - loss 0.73657775\n",
      "2019-06-18 17:41:18,464 epoch 35 - iter 31/319 - loss 0.52142772\n",
      "2019-06-18 17:41:56,574 epoch 35 - iter 62/319 - loss 0.52722048\n",
      "2019-06-18 17:42:47,140 epoch 35 - iter 93/319 - loss 0.50639054\n",
      "2019-06-18 17:43:57,449 epoch 35 - iter 124/319 - loss 0.50216613\n",
      "2019-06-18 17:44:51,331 epoch 35 - iter 155/319 - loss 0.51526351\n",
      "2019-06-18 17:45:33,315 epoch 35 - iter 186/319 - loss 0.52752876\n",
      "2019-06-18 17:46:29,496 epoch 35 - iter 217/319 - loss 0.53558201\n",
      "2019-06-18 17:47:07,580 epoch 35 - iter 248/319 - loss 0.54643987\n",
      "2019-06-18 17:47:44,571 epoch 35 - iter 279/319 - loss 0.54191561\n",
      "2019-06-18 17:48:34,705 epoch 35 - iter 310/319 - loss 0.54421593\n",
      "2019-06-18 17:48:51,356 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 17:48:51,363 EPOCH 35 done: loss 0.5455 - lr 0.0016 - bad epochs 0\n",
      "2019-06-18 17:51:10,801 DEV : loss 1.0960664749145508 - score 0.6216\n",
      "2019-06-18 17:53:03,278 TEST : loss 1.1681829690933228 - score 0.6231\n",
      "2019-06-18 17:53:09,358 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 17:53:11,085 epoch 36 - iter 0/319 - loss 0.54581946\n",
      "2019-06-18 17:53:50,042 epoch 36 - iter 31/319 - loss 0.62610077\n",
      "2019-06-18 17:54:41,116 epoch 36 - iter 62/319 - loss 0.57834729\n",
      "2019-06-18 17:55:27,917 epoch 36 - iter 93/319 - loss 0.56430522\n",
      "2019-06-18 17:56:22,505 epoch 36 - iter 124/319 - loss 0.56977901\n",
      "2019-06-18 17:57:11,097 epoch 36 - iter 155/319 - loss 0.58462516\n",
      "2019-06-18 17:58:07,279 epoch 36 - iter 186/319 - loss 0.57321692\n",
      "2019-06-18 17:58:53,022 epoch 36 - iter 217/319 - loss 0.55326599\n",
      "2019-06-18 17:59:39,916 epoch 36 - iter 248/319 - loss 0.54658734\n",
      "2019-06-18 18:00:35,230 epoch 36 - iter 279/319 - loss 0.54282897\n",
      "2019-06-18 18:01:20,836 epoch 36 - iter 310/319 - loss 0.54417616\n",
      "2019-06-18 18:01:29,432 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 18:01:29,434 EPOCH 36 done: loss 0.5384 - lr 0.0016 - bad epochs 1\n",
      "2019-06-18 18:03:48,895 DEV : loss 1.098361849784851 - score 0.6192\n",
      "2019-06-18 18:05:41,731 TEST : loss 1.1662893295288086 - score 0.6246\n",
      "2019-06-18 18:05:47,158 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 18:05:48,667 epoch 37 - iter 0/319 - loss 0.10808253\n",
      "2019-06-18 18:06:40,089 epoch 37 - iter 31/319 - loss 0.53356470\n",
      "2019-06-18 18:07:16,960 epoch 37 - iter 62/319 - loss 0.55210029\n",
      "2019-06-18 18:08:09,747 epoch 37 - iter 93/319 - loss 0.51521858\n",
      "2019-06-18 18:09:14,555 epoch 37 - iter 124/319 - loss 0.53499328\n",
      "2019-06-18 18:10:07,411 epoch 37 - iter 155/319 - loss 0.54551877\n",
      "2019-06-18 18:11:07,255 epoch 37 - iter 186/319 - loss 0.55634118\n",
      "2019-06-18 18:11:51,264 epoch 37 - iter 217/319 - loss 0.55062665\n",
      "2019-06-18 18:12:28,505 epoch 37 - iter 248/319 - loss 0.54495261\n",
      "2019-06-18 18:13:10,196 epoch 37 - iter 279/319 - loss 0.54027441\n",
      "2019-06-18 18:13:54,768 epoch 37 - iter 310/319 - loss 0.53929915\n",
      "2019-06-18 18:14:07,479 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 18:14:07,481 EPOCH 37 done: loss 0.5436 - lr 0.0016 - bad epochs 2\n",
      "2019-06-18 18:16:27,056 DEV : loss 1.0929954051971436 - score 0.621\n",
      "2019-06-18 18:18:19,849 TEST : loss 1.1643129587173462 - score 0.6279\n",
      "2019-06-18 18:18:25,251 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 18:18:27,182 epoch 38 - iter 0/319 - loss 0.57687259\n",
      "2019-06-18 18:19:27,601 epoch 38 - iter 31/319 - loss 0.46599758\n",
      "2019-06-18 18:20:14,389 epoch 38 - iter 62/319 - loss 0.46728512\n",
      "2019-06-18 18:21:03,559 epoch 38 - iter 93/319 - loss 0.53931650\n",
      "2019-06-18 18:22:02,119 epoch 38 - iter 124/319 - loss 0.54026342\n",
      "2019-06-18 18:22:50,802 epoch 38 - iter 155/319 - loss 0.53835595\n",
      "2019-06-18 18:23:36,637 epoch 38 - iter 186/319 - loss 0.54080829\n",
      "2019-06-18 18:24:13,871 epoch 38 - iter 217/319 - loss 0.52694099\n",
      "2019-06-18 18:24:54,143 epoch 38 - iter 248/319 - loss 0.53500157\n",
      "2019-06-18 18:25:41,611 epoch 38 - iter 279/319 - loss 0.53095921\n",
      "2019-06-18 18:26:30,696 epoch 38 - iter 310/319 - loss 0.53729117\n",
      "2019-06-18 18:26:43,192 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 18:26:43,193 EPOCH 38 done: loss 0.5391 - lr 0.0016 - bad epochs 3\n",
      "2019-06-18 18:29:03,985 DEV : loss 1.10160231590271 - score 0.6211\n",
      "2019-06-18 18:30:54,446 TEST : loss 1.16923987865448 - score 0.6257\n",
      "Epoch    37: reducing learning rate of group 0 to 7.8125e-04.\n",
      "2019-06-18 18:31:00,040 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 18:31:01,971 epoch 39 - iter 0/319 - loss 0.64085203\n",
      "2019-06-18 18:31:50,184 epoch 39 - iter 31/319 - loss 0.60972435\n",
      "2019-06-18 18:32:24,542 epoch 39 - iter 62/319 - loss 0.57587379\n",
      "2019-06-18 18:33:04,821 epoch 39 - iter 93/319 - loss 0.60029017\n",
      "2019-06-18 18:34:09,506 epoch 39 - iter 124/319 - loss 0.58063001\n",
      "2019-06-18 18:34:51,127 epoch 39 - iter 155/319 - loss 0.56754572\n",
      "2019-06-18 18:35:51,781 epoch 39 - iter 186/319 - loss 0.57465262\n",
      "2019-06-18 18:36:30,548 epoch 39 - iter 217/319 - loss 0.55872761\n",
      "2019-06-18 18:37:08,816 epoch 39 - iter 248/319 - loss 0.54217506\n",
      "2019-06-18 18:37:51,486 epoch 39 - iter 279/319 - loss 0.53884390\n",
      "2019-06-18 18:38:59,257 epoch 39 - iter 310/319 - loss 0.54158847\n",
      "2019-06-18 18:39:15,848 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 18:39:15,850 EPOCH 39 done: loss 0.5451 - lr 0.0008 - bad epochs 0\n",
      "2019-06-18 18:41:35,073 DEV : loss 1.0992701053619385 - score 0.6192\n",
      "2019-06-18 18:43:28,312 TEST : loss 1.1677967309951782 - score 0.6258\n",
      "2019-06-18 18:43:33,697 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 18:43:36,862 epoch 40 - iter 0/319 - loss 0.30714643\n",
      "2019-06-18 18:44:31,248 epoch 40 - iter 31/319 - loss 0.50107038\n",
      "2019-06-18 18:45:04,632 epoch 40 - iter 62/319 - loss 0.49171966\n",
      "2019-06-18 18:45:44,266 epoch 40 - iter 93/319 - loss 0.48652726\n",
      "2019-06-18 18:46:31,271 epoch 40 - iter 124/319 - loss 0.48832832\n",
      "2019-06-18 18:47:29,180 epoch 40 - iter 155/319 - loss 0.50225619\n",
      "2019-06-18 18:48:23,054 epoch 40 - iter 186/319 - loss 0.49502435\n",
      "2019-06-18 18:48:58,116 epoch 40 - iter 217/319 - loss 0.49879071\n",
      "2019-06-18 18:49:40,992 epoch 40 - iter 248/319 - loss 0.51179901\n",
      "2019-06-18 18:50:40,165 epoch 40 - iter 279/319 - loss 0.51699149\n",
      "2019-06-18 18:51:34,233 epoch 40 - iter 310/319 - loss 0.52611428\n",
      "2019-06-18 18:51:46,655 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 18:51:46,657 EPOCH 40 done: loss 0.5287 - lr 0.0008 - bad epochs 1\n",
      "2019-06-18 18:54:05,684 DEV : loss 1.0998578071594238 - score 0.6184\n",
      "2019-06-18 18:55:58,445 TEST : loss 1.1696875095367432 - score 0.6251\n",
      "2019-06-18 18:56:03,855 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 18:56:06,399 epoch 41 - iter 0/319 - loss 1.01219368\n",
      "2019-06-18 18:57:07,389 epoch 41 - iter 31/319 - loss 0.62230889\n",
      "2019-06-18 18:58:02,934 epoch 41 - iter 62/319 - loss 0.58553886\n",
      "2019-06-18 18:58:51,983 epoch 41 - iter 93/319 - loss 0.56962909\n",
      "2019-06-18 18:59:38,533 epoch 41 - iter 124/319 - loss 0.54706963\n",
      "2019-06-18 19:00:15,344 epoch 41 - iter 155/319 - loss 0.53058294\n",
      "2019-06-18 19:00:56,842 epoch 41 - iter 186/319 - loss 0.54183917\n",
      "2019-06-18 19:01:44,539 epoch 41 - iter 217/319 - loss 0.53996724\n",
      "2019-06-18 19:02:35,442 epoch 41 - iter 248/319 - loss 0.53822030\n",
      "2019-06-18 19:03:12,721 epoch 41 - iter 279/319 - loss 0.53538488\n",
      "2019-06-18 19:04:16,070 epoch 41 - iter 310/319 - loss 0.53020777\n",
      "2019-06-18 19:04:26,726 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:04:26,734 EPOCH 41 done: loss 0.5325 - lr 0.0008 - bad epochs 2\n",
      "2019-06-18 19:06:45,900 DEV : loss 1.0982688665390015 - score 0.6188\n",
      "2019-06-18 19:08:39,228 TEST : loss 1.1686756610870361 - score 0.6271\n",
      "2019-06-18 19:08:45,532 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:08:48,501 epoch 42 - iter 0/319 - loss 0.16594660\n",
      "2019-06-18 19:09:28,316 epoch 42 - iter 31/319 - loss 0.61106179\n",
      "2019-06-18 19:10:23,430 epoch 42 - iter 62/319 - loss 0.61352497\n",
      "2019-06-18 19:11:06,429 epoch 42 - iter 93/319 - loss 0.57554097\n",
      "2019-06-18 19:11:44,334 epoch 42 - iter 124/319 - loss 0.57442402\n",
      "2019-06-18 19:12:40,096 epoch 42 - iter 155/319 - loss 0.58268854\n",
      "2019-06-18 19:13:48,055 epoch 42 - iter 186/319 - loss 0.57055966\n",
      "2019-06-18 19:14:29,020 epoch 42 - iter 217/319 - loss 0.56240348\n",
      "2019-06-18 19:15:16,114 epoch 42 - iter 248/319 - loss 0.55391080\n",
      "2019-06-18 19:15:53,396 epoch 42 - iter 279/319 - loss 0.55951136\n",
      "2019-06-18 19:16:54,839 epoch 42 - iter 310/319 - loss 0.56116506\n",
      "2019-06-18 19:17:07,900 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:17:07,901 EPOCH 42 done: loss 0.5613 - lr 0.0008 - bad epochs 3\n",
      "2019-06-18 19:19:27,756 DEV : loss 1.0979182720184326 - score 0.6191\n",
      "2019-06-18 19:21:20,826 TEST : loss 1.1677608489990234 - score 0.6246\n",
      "Epoch    41: reducing learning rate of group 0 to 3.9063e-04.\n",
      "2019-06-18 19:21:27,246 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:21:29,190 epoch 43 - iter 0/319 - loss 0.28176317\n",
      "2019-06-18 19:22:26,454 epoch 43 - iter 31/319 - loss 0.53950749\n",
      "2019-06-18 19:23:10,438 epoch 43 - iter 62/319 - loss 0.55433605\n",
      "2019-06-18 19:23:49,106 epoch 43 - iter 93/319 - loss 0.55471503\n",
      "2019-06-18 19:24:29,376 epoch 43 - iter 124/319 - loss 0.53333277\n",
      "2019-06-18 19:25:27,515 epoch 43 - iter 155/319 - loss 0.51418691\n",
      "2019-06-18 19:26:18,194 epoch 43 - iter 186/319 - loss 0.52310686\n",
      "2019-06-18 19:26:53,022 epoch 43 - iter 217/319 - loss 0.50333411\n",
      "2019-06-18 19:27:45,583 epoch 43 - iter 248/319 - loss 0.51975234\n",
      "2019-06-18 19:28:34,816 epoch 43 - iter 279/319 - loss 0.52489381\n",
      "2019-06-18 19:29:36,240 epoch 43 - iter 310/319 - loss 0.52393936\n",
      "2019-06-18 19:29:47,623 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:29:47,625 EPOCH 43 done: loss 0.5272 - lr 0.0004 - bad epochs 0\n",
      "2019-06-18 19:32:07,226 DEV : loss 1.0974565744400024 - score 0.6208\n",
      "2019-06-18 19:34:00,060 TEST : loss 1.1678624153137207 - score 0.6263\n",
      "2019-06-18 19:34:05,398 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:34:08,410 epoch 44 - iter 0/319 - loss 0.80884719\n",
      "2019-06-18 19:34:53,510 epoch 44 - iter 31/319 - loss 0.51107649\n",
      "2019-06-18 19:35:40,180 epoch 44 - iter 62/319 - loss 0.54397367\n",
      "2019-06-18 19:36:19,641 epoch 44 - iter 93/319 - loss 0.54927881\n",
      "2019-06-18 19:37:05,240 epoch 44 - iter 124/319 - loss 0.55440282\n",
      "2019-06-18 19:37:53,946 epoch 44 - iter 155/319 - loss 0.54506259\n",
      "2019-06-18 19:38:33,317 epoch 44 - iter 186/319 - loss 0.54165545\n",
      "2019-06-18 19:39:37,835 epoch 44 - iter 217/319 - loss 0.55063545\n",
      "2019-06-18 19:40:35,136 epoch 44 - iter 248/319 - loss 0.55291678\n",
      "2019-06-18 19:41:19,233 epoch 44 - iter 279/319 - loss 0.54282624\n",
      "2019-06-18 19:42:08,492 epoch 44 - iter 310/319 - loss 0.54372067\n",
      "2019-06-18 19:42:22,884 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:42:22,892 EPOCH 44 done: loss 0.5441 - lr 0.0004 - bad epochs 1\n",
      "2019-06-18 19:44:42,549 DEV : loss 1.0974047183990479 - score 0.6221\n",
      "2019-06-18 19:46:35,596 TEST : loss 1.168610692024231 - score 0.623\n",
      "2019-06-18 19:46:40,930 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:46:43,328 epoch 45 - iter 0/319 - loss 0.33143166\n",
      "2019-06-18 19:47:20,026 epoch 45 - iter 31/319 - loss 0.58669874\n",
      "2019-06-18 19:47:57,018 epoch 45 - iter 62/319 - loss 0.55172090\n",
      "2019-06-18 19:48:43,066 epoch 45 - iter 93/319 - loss 0.56174028\n",
      "2019-06-18 19:49:47,056 epoch 45 - iter 124/319 - loss 0.54042891\n",
      "2019-06-18 19:50:45,686 epoch 45 - iter 155/319 - loss 0.55036841\n",
      "2019-06-18 19:51:24,722 epoch 45 - iter 186/319 - loss 0.54856081\n",
      "2019-06-18 19:52:06,456 epoch 45 - iter 217/319 - loss 0.55897505\n",
      "2019-06-18 19:53:09,495 epoch 45 - iter 248/319 - loss 0.56499495\n",
      "2019-06-18 19:53:48,688 epoch 45 - iter 279/319 - loss 0.55757177\n",
      "2019-06-18 19:54:48,225 epoch 45 - iter 310/319 - loss 0.54659242\n",
      "2019-06-18 19:54:59,202 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:54:59,206 EPOCH 45 done: loss 0.5465 - lr 0.0004 - bad epochs 2\n",
      "2019-06-18 19:57:21,356 DEV : loss 1.0970677137374878 - score 0.6215\n",
      "2019-06-18 19:59:14,797 TEST : loss 1.1680586338043213 - score 0.6228\n",
      "2019-06-18 19:59:20,422 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:59:22,328 epoch 46 - iter 0/319 - loss 0.48935401\n",
      "2019-06-18 20:00:08,366 epoch 46 - iter 31/319 - loss 0.54652302\n",
      "2019-06-18 20:00:51,395 epoch 46 - iter 62/319 - loss 0.51669958\n",
      "2019-06-18 20:01:33,526 epoch 46 - iter 93/319 - loss 0.52306574\n",
      "2019-06-18 20:02:17,918 epoch 46 - iter 124/319 - loss 0.51534580\n",
      "2019-06-18 20:03:18,366 epoch 46 - iter 155/319 - loss 0.50985177\n",
      "2019-06-18 20:03:55,290 epoch 46 - iter 186/319 - loss 0.50833128\n",
      "2019-06-18 20:04:38,199 epoch 46 - iter 217/319 - loss 0.50822318\n",
      "2019-06-18 20:05:38,394 epoch 46 - iter 248/319 - loss 0.52331692\n",
      "2019-06-18 20:06:37,896 epoch 46 - iter 279/319 - loss 0.53206549\n",
      "2019-06-18 20:07:34,155 epoch 46 - iter 310/319 - loss 0.54142442\n",
      "2019-06-18 20:07:41,378 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:07:41,383 EPOCH 46 done: loss 0.5406 - lr 0.0004 - bad epochs 3\n",
      "2019-06-18 20:10:00,503 DEV : loss 1.0967742204666138 - score 0.6208\n",
      "2019-06-18 20:11:53,690 TEST : loss 1.1670280694961548 - score 0.6237\n",
      "Epoch    45: reducing learning rate of group 0 to 1.9531e-04.\n",
      "2019-06-18 20:11:59,473 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:12:02,037 epoch 47 - iter 0/319 - loss 0.97317100\n",
      "2019-06-18 20:12:57,229 epoch 47 - iter 31/319 - loss 0.53062922\n",
      "2019-06-18 20:13:35,309 epoch 47 - iter 62/319 - loss 0.51488555\n",
      "2019-06-18 20:14:21,344 epoch 47 - iter 93/319 - loss 0.55870109\n",
      "2019-06-18 20:14:58,460 epoch 47 - iter 124/319 - loss 0.56629509\n",
      "2019-06-18 20:15:36,136 epoch 47 - iter 155/319 - loss 0.53727375\n",
      "2019-06-18 20:16:20,528 epoch 47 - iter 186/319 - loss 0.53525776\n",
      "2019-06-18 20:17:28,650 epoch 47 - iter 217/319 - loss 0.54972410\n",
      "2019-06-18 20:18:13,246 epoch 47 - iter 248/319 - loss 0.54138272\n",
      "2019-06-18 20:18:56,607 epoch 47 - iter 279/319 - loss 0.53744077\n",
      "2019-06-18 20:20:04,877 epoch 47 - iter 310/319 - loss 0.53224183\n",
      "2019-06-18 20:20:15,631 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:20:15,633 EPOCH 47 done: loss 0.5335 - lr 0.0002 - bad epochs 0\n",
      "2019-06-18 20:22:35,508 DEV : loss 1.0969220399856567 - score 0.6211\n",
      "2019-06-18 20:24:28,556 TEST : loss 1.1671459674835205 - score 0.6238\n",
      "2019-06-18 20:24:34,102 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:24:36,099 epoch 48 - iter 0/319 - loss 0.34119838\n",
      "2019-06-18 20:25:18,774 epoch 48 - iter 31/319 - loss 0.56980980\n",
      "2019-06-18 20:26:03,780 epoch 48 - iter 62/319 - loss 0.52690617\n",
      "2019-06-18 20:27:12,895 epoch 48 - iter 93/319 - loss 0.53451357\n",
      "2019-06-18 20:28:00,899 epoch 48 - iter 124/319 - loss 0.52622620\n",
      "2019-06-18 20:28:49,699 epoch 48 - iter 155/319 - loss 0.52792787\n",
      "2019-06-18 20:29:37,006 epoch 48 - iter 186/319 - loss 0.52836933\n",
      "2019-06-18 20:30:16,503 epoch 48 - iter 217/319 - loss 0.53375325\n",
      "2019-06-18 20:31:04,532 epoch 48 - iter 248/319 - loss 0.52805812\n",
      "2019-06-18 20:31:58,790 epoch 48 - iter 279/319 - loss 0.53670868\n",
      "2019-06-18 20:32:46,692 epoch 48 - iter 310/319 - loss 0.54045023\n",
      "2019-06-18 20:32:58,352 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:32:58,354 EPOCH 48 done: loss 0.5369 - lr 0.0002 - bad epochs 1\n",
      "2019-06-18 20:35:18,922 DEV : loss 1.0970426797866821 - score 0.6211\n",
      "2019-06-18 20:37:12,794 TEST : loss 1.1670715808868408 - score 0.6238\n",
      "2019-06-18 20:37:18,399 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:37:20,984 epoch 49 - iter 0/319 - loss 0.77290970\n",
      "2019-06-18 20:37:59,824 epoch 49 - iter 31/319 - loss 0.54098469\n",
      "2019-06-18 20:38:43,077 epoch 49 - iter 62/319 - loss 0.53848068\n",
      "2019-06-18 20:39:28,269 epoch 49 - iter 93/319 - loss 0.51062725\n",
      "2019-06-18 20:40:12,776 epoch 49 - iter 124/319 - loss 0.52269325\n",
      "2019-06-18 20:41:09,739 epoch 49 - iter 155/319 - loss 0.55868928\n",
      "2019-06-18 20:42:09,651 epoch 49 - iter 186/319 - loss 0.55951177\n",
      "2019-06-18 20:42:58,215 epoch 49 - iter 217/319 - loss 0.55991625\n",
      "2019-06-18 20:43:50,335 epoch 49 - iter 248/319 - loss 0.55821196\n",
      "2019-06-18 20:44:46,115 epoch 49 - iter 279/319 - loss 0.55677670\n",
      "2019-06-18 20:45:27,482 epoch 49 - iter 310/319 - loss 0.54836798\n",
      "2019-06-18 20:45:39,196 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:45:39,197 EPOCH 49 done: loss 0.5462 - lr 0.0002 - bad epochs 2\n",
      "2019-06-18 20:48:00,325 DEV : loss 1.0979206562042236 - score 0.6201\n",
      "2019-06-18 20:49:54,330 TEST : loss 1.167771339416504 - score 0.624\n",
      "2019-06-18 20:50:00,686 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:50:02,907 epoch 50 - iter 0/319 - loss 0.41679555\n",
      "2019-06-18 20:50:57,584 epoch 50 - iter 31/319 - loss 0.66788617\n",
      "2019-06-18 20:51:49,122 epoch 50 - iter 62/319 - loss 0.59999607\n",
      "2019-06-18 20:52:35,069 epoch 50 - iter 93/319 - loss 0.58887010\n",
      "2019-06-18 20:53:20,299 epoch 50 - iter 124/319 - loss 0.55531492\n",
      "2019-06-18 20:54:14,495 epoch 50 - iter 155/319 - loss 0.55301112\n",
      "2019-06-18 20:55:11,471 epoch 50 - iter 186/319 - loss 0.56773363\n",
      "2019-06-18 20:55:57,614 epoch 50 - iter 217/319 - loss 0.54603191\n",
      "2019-06-18 20:56:34,317 epoch 50 - iter 248/319 - loss 0.55155487\n",
      "2019-06-18 20:57:06,711 epoch 50 - iter 279/319 - loss 0.54607179\n",
      "2019-06-18 20:58:12,964 epoch 50 - iter 310/319 - loss 0.54267835\n",
      "2019-06-18 20:58:31,551 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:58:31,555 EPOCH 50 done: loss 0.5461 - lr 0.0002 - bad epochs 3\n",
      "2019-06-18 21:00:53,306 DEV : loss 1.0975849628448486 - score 0.6214\n",
      "2019-06-18 21:02:47,827 TEST : loss 1.167595386505127 - score 0.6246\n",
      "Epoch    49: reducing learning rate of group 0 to 9.7656e-05.\n",
      "2019-06-18 21:02:53,578 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 21:02:53,586 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 21:02:53,591 learning rate too small - quitting training!\n",
      "2019-06-18 21:02:53,596 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 21:03:02,454 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 21:03:02,457 Testing using best model ...\n",
      "2019-06-18 21:03:02,463 loading file resources/taggers/resume-ner-1-nd/best-model.pt\n",
      "2019-06-18 21:05:04,249 0.6608\t0.5952\t0.6263\n",
      "2019-06-18 21:05:04,251 \n",
      "MICRO_AVG: acc 0.4559 - f1-score 0.6263\n",
      "MACRO_AVG: acc 0.3844 - f1-score 0.514190909090909\n",
      "\"B-Companies tp: 239 - fp: 94 - fn: 69 - tn: 239 - precision: 0.7177 - recall: 0.7760 - accuracy: 0.5945 - f1-score: 0.7457\n",
      "\"I-Companies tp: 419 - fp: 163 - fn: 74 - tn: 419 - precision: 0.7199 - recall: 0.8499 - accuracy: 0.6387 - f1-score: 0.7795\n",
      "\"L-Companies tp: 247 - fp: 89 - fn: 69 - tn: 247 - precision: 0.7351 - recall: 0.7816 - accuracy: 0.6099 - f1-score: 0.7576\n",
      "\"U-Companies tp: 9 - fp: 35 - fn: 81 - tn: 9 - precision: 0.2045 - recall: 0.1000 - accuracy: 0.0720 - f1-score: 0.1343\n",
      "-          tp: 12 - fp: 48 - fn: 430 - tn: 12 - precision: 0.2000 - recall: 0.0271 - accuracy: 0.0245 - f1-score: 0.0477\n",
      "Degree     tp: 42 - fp: 36 - fn: 31 - tn: 42 - precision: 0.5385 - recall: 0.5753 - accuracy: 0.3853 - f1-score: 0.5563\n",
      "Designation tp: 231 - fp: 147 - fn: 116 - tn: 231 - precision: 0.6111 - recall: 0.6657 - accuracy: 0.4676 - f1-score: 0.6372\n",
      "L-Degree   tp: 40 - fp: 27 - fn: 27 - tn: 40 - precision: 0.5970 - recall: 0.5970 - accuracy: 0.4255 - f1-score: 0.5970\n",
      "L-Designation tp: 243 - fp: 119 - fn: 85 - tn: 243 - precision: 0.6713 - recall: 0.7409 - accuracy: 0.5436 - f1-score: 0.7044\n",
      "U-Degree   tp: 10 - fp: 4 - fn: 11 - tn: 10 - precision: 0.7143 - recall: 0.4762 - accuracy: 0.4000 - f1-score: 0.5714\n",
      "U-Designation tp: 2 - fp: 5 - fn: 23 - tn: 2 - precision: 0.2857 - recall: 0.0800 - accuracy: 0.0667 - f1-score: 0.1250\n",
      "2019-06-18 21:05:04,260 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dev_loss_history': [tensor(1.0936, device='cuda:0'),\n",
       "  tensor(1.0961, device='cuda:0'),\n",
       "  tensor(1.0984, device='cuda:0'),\n",
       "  tensor(1.0930, device='cuda:0'),\n",
       "  tensor(1.1016, device='cuda:0'),\n",
       "  tensor(1.0993, device='cuda:0'),\n",
       "  tensor(1.0999, device='cuda:0'),\n",
       "  tensor(1.0983, device='cuda:0'),\n",
       "  tensor(1.0979, device='cuda:0'),\n",
       "  tensor(1.0975, device='cuda:0'),\n",
       "  tensor(1.0974, device='cuda:0'),\n",
       "  tensor(1.0971, device='cuda:0'),\n",
       "  tensor(1.0968, device='cuda:0'),\n",
       "  tensor(1.0969, device='cuda:0'),\n",
       "  tensor(1.0970, device='cuda:0'),\n",
       "  tensor(1.0979, device='cuda:0'),\n",
       "  tensor(1.0976, device='cuda:0')],\n",
       " 'dev_score_history': [0.6245,\n",
       "  0.6216,\n",
       "  0.6192,\n",
       "  0.621,\n",
       "  0.6211,\n",
       "  0.6192,\n",
       "  0.6184,\n",
       "  0.6188,\n",
       "  0.6191,\n",
       "  0.6208,\n",
       "  0.6221,\n",
       "  0.6215,\n",
       "  0.6208,\n",
       "  0.6211,\n",
       "  0.6211,\n",
       "  0.6201,\n",
       "  0.6214],\n",
       " 'test_score': 0.6263,\n",
       " 'train_loss_history': [0.5428262556608194,\n",
       "  0.5454841540151255,\n",
       "  0.5383724374550637,\n",
       "  0.5435990104013849,\n",
       "  0.5390514056417262,\n",
       "  0.5450503251115356,\n",
       "  0.528659676589936,\n",
       "  0.5325136141250126,\n",
       "  0.5612693150589085,\n",
       "  0.5272064603701654,\n",
       "  0.5440840239528578,\n",
       "  0.546471429562494,\n",
       "  0.5405576271603474,\n",
       "  0.5335354520048841,\n",
       "  0.536924873867005,\n",
       "  0.5462127741788249,\n",
       "  0.5461213372437558]}"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.training_utils import EvaluationMetric\n",
    "\n",
    "## continue trainng if aborted\n",
    "checkpoint = tagger.load_checkpoint(Path('resources/taggers/resume-ner-1-nd/checkpoint.pt'))\n",
    "trainer = ModelTrainer.load_from_checkpoint(checkpoint, corpus)\n",
    "trainer.train('resources/taggers/resume-ner-1-nd',\n",
    "              EvaluationMetric.MICRO_F1_SCORE,\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=150,\n",
    "              checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQZlLP3HJOen"
   },
   "outputs": [],
   "source": [
    "from flair.visual.training_curves import Plotter\n",
    "\n",
    "## plot training curves and weights\n",
    "plotter = Plotter()\n",
    "plotter.plot_training_curves('./resources/taggers/resume-ner-1/loss.tsv')\n",
    "plotter.plot_weights('./resources/taggers/resume-ner-1/weights.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 47036,
     "status": "ok",
     "timestamp": 1560925439986,
     "user": {
      "displayName": "Armin Roth",
      "photoUrl": "",
      "userId": "08591799635376506733"
     },
     "user_tz": -120
    },
    "id": "J6aV_qJbRUns",
    "outputId": "d0da5ec8-3703-4adc-f472-7f4e746a2ca1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For label 'Degree' tp: 201 fp: 88 fn: 133\n",
      "For label 'Companies' tp: 874 fp: 333 fn: 363\n",
      "For label 'Designation' tp: 665 fp: 258 fn: 353\n",
      "Entity-Level evaluation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Degree</th>\n",
       "      <td>0.695502</td>\n",
       "      <td>0.601796</td>\n",
       "      <td>0.645265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Companies worked at</th>\n",
       "      <td>0.724109</td>\n",
       "      <td>0.706548</td>\n",
       "      <td>0.715221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Designation</th>\n",
       "      <td>0.720477</td>\n",
       "      <td>0.653242</td>\n",
       "      <td>0.685214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg</th>\n",
       "      <td>0.713363</td>\n",
       "      <td>0.653862</td>\n",
       "      <td>0.681900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     precision    recall        f1\n",
       "Degree                0.695502  0.601796  0.645265\n",
       "Companies worked at   0.724109  0.706548  0.715221\n",
       "Designation           0.720477  0.653242  0.685214\n",
       "avg                   0.713363  0.653862  0.681900"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "## clculate entity-level evaluation metrics \n",
    "\n",
    "search_string = ['Degree', 'Companies', 'Designation'] \n",
    "test = pd.read_csv('./resources/taggers/resume-ner-1-nd/test.tsv', delim_whitespace=True, engine=\"python\",names=['Text','Predicted','True','val'])\n",
    "\n",
    "data = []\n",
    "for label in search_string:\n",
    "    # variables to store results for all resumes for one entity type\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    for _,tres_df in test.iterrows():\n",
    "        # calculate true false positives and false negatives for each resume\n",
    "        if (tres_df[\"Predicted\"] == tres_df[\"True\"]) & (label in tres_df[\"Predicted\"]): \n",
    "          true_positives += 1\n",
    "        if (tres_df[\"Predicted\"] != tres_df[\"True\"]) & (label in tres_df[\"Predicted\"]):\n",
    "          false_positives += 1\n",
    "        if (tres_df[\"Predicted\"] != tres_df[\"True\"]) & (label in tres_df[\"True\"]):\n",
    "          false_negatives += 1\n",
    "        \n",
    "    \n",
    "    print(\"For label '{}' tp: {} fp: {} fn: {}\".format(label,true_positives,false_positives,false_negatives))\n",
    "\n",
    "    precision = 0.0 if true_positives == 0 else float(true_positives) / (true_positives + false_positives)\n",
    "    recall =  0.0 if true_positives == 0 else float(true_positives) / (true_positives + false_negatives)\n",
    "    f1 =  0.0 if true_positives == 0 else 2 * ((precision * recall)/(precision + recall))\n",
    "    row = [precision,recall,f1]\n",
    "    data.append(row)\n",
    "    \n",
    "\n",
    "metric_df = pd.DataFrame(data, columns = ['precision', 'recall', 'f1'], index = ['Degree', 'Companies worked at', 'Designation']) \n",
    "metric_df.loc['avg'] = metric_df.mean()\n",
    "\n",
    "print(\"Entity-Level evaluation:\")\n",
    "\n",
    "display(metric_df)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "flair_nlp_colab.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
