{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume NER\n",
    "## Extract Information from Resumes using Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration and Preselection\n",
    "In this notebook the dataset is loaded and examined.\n",
    "Furthermore the features to train are selected and filtered out from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701\n",
      "690\n",
      "532\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "import json\n",
    "\n",
    "dataset_path = \"./data/Entity Recognition in Resumes.json\"\n",
    "\n",
    "## open file and convert resume entries to json\n",
    "with open(dataset_path,encoding=\"utf8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "all_resumes = [json.loads(line) for line in lines]\n",
    "\n",
    "\n",
    "## data conversion method\n",
    "def convert_data(data):\n",
    "    \"\"\"\n",
    "    Creates NER training data in Spacy format from JSON dataset\n",
    "    Outputs the Spacy training data which can be used for Spacy training.\n",
    "    \"\"\"\n",
    "    text = data['content']\n",
    "    entities = []\n",
    "    if data['annotation'] is not None:\n",
    "        for annotation in data['annotation']:\n",
    "            # only a single point in text annotation.\n",
    "            point = annotation['points'][0]\n",
    "            labels = annotation['label']\n",
    "            # handle both list of labels or a single label.\n",
    "            if not isinstance(labels, list):\n",
    "                labels = [labels]\n",
    "            for label in labels:\n",
    "                # dataturks indices are both inclusive [start, end] but spacy is not [start, end)\n",
    "                entities.append((point['start'], point['end'] + 1, label))\n",
    "    return (text, {\"entities\": entities})\n",
    "   \n",
    "##  convert all resumes to spacy format\n",
    "converted_resumes = [convert_data(resume) for resume in all_resumes]\n",
    "print(len(converted_resumes))\n",
    "\n",
    "## filter out resumes where resume entities list is None\n",
    "converted_resumes = [i for i in converted_resumes if i[1]['entities']] \n",
    "print(len(converted_resumes))\n",
    "\n",
    "## filter out duplicate resumes tagged differently\n",
    "\n",
    "unique_resumes = []\n",
    "unique_resumes_text = set()\n",
    "for res in converted_resumes:\n",
    "    if res[0] not in unique_resumes_text:\n",
    "        unique_resumes.append(res)\n",
    "        unique_resumes_text.add(res[0])\n",
    "print(len(unique_resumes)) \n",
    "#converted_resumes = unique_resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSkills\u001b[0m :  SKILL SET • ASP.NET, C# • QA tools\n",
      "\n",
      "• Coding and modularization • Excellent communication skills\n",
      "\n",
      "• VB, VB.net, ASP • Technical specifications creation\n",
      "\n",
      "• HTML • System backups\n",
      "\n",
      "• Sql server 2005, Oracle • System upgrades\n",
      "\n",
      "• Java/C/C++ • Excellent problem-solving abilities\n",
      "\n",
      "Navas Najeer Koya 3\n",
      "\u001b[1mLocation\u001b[0m :  Mangalore\n",
      "\u001b[1mSkills\u001b[0m :  C# (Less than 1 year), .NET, SQL Server, Css, Html5\n",
      "\n",
      "\u001b[1mGraduation Year\u001b[0m :   2014\n",
      "\u001b[1mLocation\u001b[0m :  Mangalore\n",
      "\u001b[1mLocation\u001b[0m :  Mangalore\n",
      "\u001b[1mDegree\u001b[0m :  Bachelor of Computer Application\n",
      "\u001b[1mGraduation Year\u001b[0m :   2014\n",
      "\u001b[1mCompanies worked at\u001b[0m :  Infosys\n",
      "\u001b[1mDesignation\u001b[0m :  Test Engineer\n",
      "\n",
      "\u001b[1mCompanies worked at\u001b[0m :  Infosys\n",
      "\u001b[1mDesignation\u001b[0m :  Test Engineer\n",
      "\n",
      "\u001b[1mGraduation Year\u001b[0m :   2014\n",
      "\u001b[1mCompanies worked at\u001b[0m :  Infosys\n",
      "\u001b[1mDesignation\u001b[0m :  System Engineer\n",
      "\u001b[1mLocation\u001b[0m :  Mangalore\n",
      "\u001b[1mLocation\u001b[0m :  Mangalore\n",
      "\u001b[1mDesignation\u001b[0m :  Test Engineer\n",
      "\n",
      "\u001b[1mName\u001b[0m :  Navas Koya\n"
     ]
    }
   ],
   "source": [
    "## print one sample resume for better understanding of data\n",
    "converted_resume = converted_resumes[42]\n",
    "\n",
    "text = converted_resume[0]\n",
    "entities_list = converted_resume[1]['entities']\n",
    "## print label and text for each entity\n",
    "for entity in entities_list:\n",
    "    print('\\033[1m' + entity[2] + '\\033[0m', \": \", text[entity[0]:entity[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graduation Year\n",
      "Companies worked at\n",
      "University\n",
      "abc\n",
      "Certifications\n",
      "Can Relocate to\n",
      "Links\n",
      "links\n",
      "Name\n",
      "Years of Experience\n",
      "Degree\n",
      "training\n",
      "Skills\n",
      "state\n",
      "Location\n",
      "Relocate to\n",
      "projects\n",
      "College\n",
      "des\n",
      "Rewards and Achievements\n",
      "UNKNOWN\n",
      "College Name\n",
      "Designation\n",
      "Email Address\n",
      "Address\n"
     ]
    }
   ],
   "source": [
    "## collect names of all entities in complete resume dataset\n",
    "all_labels = list()\n",
    "for res in converted_resumes:\n",
    "    ## entity list of res\n",
    "    entity_list = [i[2] for i in res[1]['entities']]\n",
    "    ## extend all_labels with labels of entities \n",
    "    all_labels += entity_list\n",
    "    \n",
    "## set of unique values\n",
    "unique_labels = list(set(all_labels))\n",
    "## print labels to collect the right labels for later training\n",
    "x = [print(i) for i in unique_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs with Degree: 606\n",
      "Total count of Degree: 1012\n",
      "Docs with Companies worked at: 627\n",
      "Total count of Companies worked at: 2830\n",
      "Docs with Designation: 650\n",
      "Total count of Designation: 2842\n",
      "Docs total: 690\n"
     ]
    }
   ],
   "source": [
    "chosen_entity_labels = ['Degree','Companies worked at', 'Designation']\n",
    "## for each chosen entity label, count how many documents have a labeled entity for that label, and how many labeled entities total there are \n",
    "\n",
    "for chosen in chosen_entity_labels:\n",
    "    found_docs_with_entity = 0\n",
    "    entity_count = 0\n",
    "    for resume in converted_resumes:\n",
    "        entity_list = resume[1][\"entities\"]\n",
    "        _,_,labels = zip(*entity_list)\n",
    "        if chosen in labels:\n",
    "            found_docs_with_entity+=1\n",
    "            entity_count+=len([l for l in labels if l == chosen])\n",
    "    print(\"Docs with {}: {}\".format(chosen,found_docs_with_entity))\n",
    "    print(\"Total count of {}: {}\".format(chosen,entity_count))\n",
    "print(\"Docs total: {}\".format(len(converted_resumes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathered 547 training examples\n"
     ]
    }
   ],
   "source": [
    "resumes = converted_resumes\n",
    "\n",
    "## this method gathers all resumes which have all of the chosen entites above.\n",
    "def gather_candidates(dataset,entity_labels):\n",
    "    candidates = list()\n",
    "    for resume in dataset:\n",
    "        res_ent_labels = list(zip(*resume[1][\"entities\"]))[2]\n",
    "        if set(entity_labels).issubset(res_ent_labels):\n",
    "            candidates.append(resume)\n",
    "    return candidates\n",
    "\n",
    "training_data = gather_candidates(resumes, chosen_entity_labels)\n",
    "print(\"Gathered {} training examples\".format(len(training_data)))\n",
    "\n",
    "## filter all annotation based on filter list\n",
    "def filter_ents(ents, filter):\n",
    "    filtered = [ent for ent in ents if ent[2] in filter]\n",
    "    return filtered\n",
    "\n",
    "## remove all but relevant (chosen) entity annotations and store in X variable \n",
    "X = [[i[0], dict(entities=filter_ents(i[1][\"entities\"], chosen_entity_labels))] for i in training_data]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Exception thrown when processing doc:\n",
      "('Nida Khan\\nTech Support Executive - Teleperformance for Microsoft\\n\\nJaipur, Rajasthan - Email me on Indeed: indeed.com/r/Nida-Khan/6c9160696f57efd8\\n\\n• To be an integral part of the organization and enhance my knowledge to utilize it in a productive\\nmanner for the growth of the company and the global.\\n\\nINDUSTRIAL TRAINING\\n\\n• BHEL, (HEEP) HARIDWAR\\nOn CNC System&amp; PLC Programming.\\n\\nWORK EXPERIENCE\\n\\nTech Support Executive\\n\\nTeleperformance for Microsoft -\\n\\nSeptember 2017 to Present\\n\\nprocess.\\n• 21 months of experience in ADFC as Phone Banker.\\n\\nEDUCATION\\n\\nBachelor of Technology in Electronics & communication Engg\\n\\nGNIT institute of Technology -  Lucknow, Uttar Pradesh\\n\\n2008 to 2012\\n\\nClass XII\\n\\nU.P. Board -  Bareilly, Uttar Pradesh\\n\\n2007\\n\\nClass X\\n\\nU.P. Board -  Bareilly, Uttar Pradesh\\n\\n2005\\n\\nSKILLS\\n\\nMicrosoft office, excel, cisco, c language, cbs. (4 years)\\n\\nhttps://www.indeed.com/r/Nida-Khan/6c9160696f57efd8?isid=rex-download&ikw=download-top&co=IN',) ({'entities': [(552, 610, 'Degree'), (420, 449, 'Companies worked at'), (395, 418, 'Designation'), (35, 64, 'Companies worked at'), (10, 33, 'Designation'), (9, 32, 'Designation')]},)\n",
      "Losses {'ner': 50598.291109917394}\n",
      "Unfiltered training data size:  547\n",
      "Filtered training data size:  546\n",
      "Bad data size:  1\n"
     ]
    }
   ],
   "source": [
    "from helpers.spacy_train_resume_ner import train_spacy_ner\n",
    "\n",
    "def remove_bad_data(training_data):\n",
    "    model, baddocs = train_spacy_ner(training_data, debug=True, n_iter=1)\n",
    "    # training data is list of lists with each list containing a text and annotations\n",
    "    # baddocs is a set of strings/resume texts.\n",
    "    # filter bad docs and store filter result (good docs) in filtered variable\n",
    "    filtered = [data for data in training_data if data[0] not in baddocs]\n",
    "    print(\"Unfiltered training data size: \",len(training_data))\n",
    "    print(\"Filtered training data size: \", len(filtered))\n",
    "    print(\"Bad data size: \", len(baddocs))\n",
    "    return filtered\n",
    "\n",
    "## remove faulty documents that will throw errors in spacy training\n",
    "X = remove_bad_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all:   546\n",
      "test:   81\n",
      "valid: 111\n",
      "train: 354\n"
     ]
    }
   ],
   "source": [
    "## split to test train validation data\n",
    "def train_test_split(X, train_percent, test_percent):\n",
    "    train_size = int(len(X) * train_percent)\n",
    "    test_size = int(len(X) * test_percent)\n",
    "    \n",
    "    train = X[:train_size]\n",
    "    tmp = X[train_size:]\n",
    "    \n",
    "    test = tmp[:test_size]\n",
    "    valid = tmp[test_size:]\n",
    "    \n",
    "    return train, test, valid\n",
    "#0.7 train, 0.2 dev, 0,1 test\n",
    "train, test, valid = train_test_split(X, 0.65, 0.15)\n",
    "print('all:  ',len(X))\n",
    "print('test:  ',len(test))\n",
    "print('valid:',len(valid))\n",
    "print('train:',len(train))\n",
    "assert(len(train) + len(test) + len(valid) == len(X)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Losses {'ner': 28189.021530729293}\n",
      "Losses {'ner': 13750.925065617488}\n",
      "Losses {'ner': 24827.47379087636}\n",
      "Losses {'ner': 18116.78973425692}\n",
      "Losses {'ner': 26471.958610855974}\n",
      "Losses {'ner': 20526.388423813136}\n",
      "Losses {'ner': 18653.082368638366}\n",
      "Losses {'ner': 16315.142706626793}\n",
      "Losses {'ner': 12524.617815063197}\n",
      "Losses {'ner': 8341.22201218992}\n",
      "Losses {'ner': 6415.795565967112}\n",
      "Losses {'ner': 6240.065474748248}\n",
      "Losses {'ner': 5366.414414152686}\n",
      "Losses {'ner': 4760.398092845442}\n",
      "Losses {'ner': 4407.362758204549}\n",
      "Losses {'ner': 4245.000548476351}\n",
      "Losses {'ner': 3710.4525312784963}\n",
      "Losses {'ner': 3781.0111048796603}\n",
      "Losses {'ner': 3385.3498940029613}\n",
      "Losses {'ner': 3367.0447317343696}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from spacy.gold import biluo_tags_from_offsets\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "custom_nlp,_= train_spacy_ner(train,n_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "## convert data to bilou format\n",
    "def make_bilou_df(nlp,resume):\n",
    "    \"\"\"\n",
    "    param nlp - a trained spacy model\n",
    "    param resume - a resume from our train or test set\n",
    "    \"\"\"\n",
    "    doc = nlp(resume[0])\n",
    "    bilou_ents_predicted = biluo_tags_from_offsets(doc, [(ent.start_char,ent.end_char,ent.label_)for ent in doc.ents])\n",
    "    bilou_ents_true = biluo_tags_from_offsets(doc, [(ent[0], ent[1], ent[2]) for ent in resume[1][\"entities\"]])\n",
    "    \n",
    "    doc_tokens = [tok.text for tok in doc]\n",
    "    bilou_df = pd.DataFrame()\n",
    "    bilou_df[\"Tokens\"] =doc_tokens\n",
    "    bilou_df[\"Tokens\"] = bilou_df[\"Tokens\"].str.replace(\"\\\\s+\",\"\") \n",
    "    bilou_df[\"Predicted\"] = bilou_ents_predicted\n",
    "    bilou_df[\"True\"] = bilou_ents_true\n",
    "    return bilou_df \n",
    "\n",
    "## convet data to flair format\n",
    "def clear_data(data_as_bilou):\n",
    "    file = ''\n",
    "    for idx,df in enumerate(data_as_bilou):\n",
    "        df2 = pd.DataFrame()\n",
    "        df2[\"text\"] = df[\"Tokens\"]\n",
    "        df2[\"ner\"] = df[\"True\"]\n",
    "\n",
    "        # remove unwanted whitespace and/or newline token rows from dataframe\n",
    "        df2 = df2[df2.text.str.strip() != '']\n",
    "        df2 = df2[df2.text.str.strip() != '\\n']\n",
    "        df2 = df2[df2.text.str.strip() != '\\r']\n",
    "        #reset index since columns were removed\n",
    "        df2 = df2.reset_index(drop=True)\n",
    "\n",
    "        # insert newlines after each \"sentence\" and convert to csv\n",
    "        indexes = df2.index[df2.text.str.strip() == '.'].tolist()\n",
    "        indexes +=(df2.index[df2.text.str.strip() == '•'].tolist())\n",
    "        Indexes=sorted(indexes)\n",
    "        l_mod = [0] + indexes + [max(indexes,  default=0) + 1]\n",
    "        list_of_dfs = [df2.iloc[l_mod[n]:l_mod[n + 1]] for n in range(len(l_mod)-1)]\n",
    "\n",
    "        as_csv_s = [i.to_csv(None,sep = \" \",encoding = \"utf-8\",index = False,header = False,line_terminator=\"\\n\") for i in list_of_dfs]\n",
    "        \n",
    "        for i in as_csv_s:\n",
    "            file += i + \"\\n\"\n",
    "    return file\n",
    "\n",
    "# make .csv strings from train, test, valid for use in flair \n",
    "def schema_for_flair(nlp, train, test, valid):    \n",
    "    # makes a list of pandas dataframes, one for each resume. \n",
    "    training_data_as_bilou = [make_bilou_df(nlp,res) for res in train]\n",
    "    test_data_as_bilou = [make_bilou_df(nlp,res) for res in test]\n",
    "    valid_data_as_bilou = [make_bilou_df(nlp,res) for res in valid]\n",
    "    \n",
    "    \n",
    "    # return variables\n",
    "    training_file = clear_data(training_data_as_bilou)\n",
    "    test_file = clear_data(test_data_as_bilou)\n",
    "    valid_file = clear_data(valid_data_as_bilou)\n",
    "   \n",
    "    return training_file, test_file, valid_file\n",
    "\n",
    "training,testing,validation = schema_for_flair(custom_nlp,train,test,valid)\n",
    "\n",
    "with open(\"./data/train_res_bilou_f.txt\",'w+',encoding=\"utf-8\") as f: \n",
    "    f.write(training)\n",
    "with open(\"./data/test_res_bilou_f.txt\",'w+',encoding=\"utf-8\") as f: \n",
    "    f.write(testing)\n",
    "with open(\"./data/valid_res_bilou_f.txt\",'w+',encoding=\"utf-8\") as f: \n",
    "    f.write(validation)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
